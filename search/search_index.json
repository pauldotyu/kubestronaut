{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":""},{"location":"#overview","title":"Overview","text":"<p>So you want to be a Kubestronaut? You've landed in the right place!</p> <p>Passing five Kubernetes exams sounds challenging but intrinsically rewarding. Earning a Kubernetes certification is empowering, especially if you're new to the technology. It demonstrates your ability to deploy, manage, and secure Kubernetes environments, which are essential skills for any cloud native engineer. </p> <p>The path to Kubestronaut is definitely an arduous journey that requires dedication, hands-on practice, and a structured approach. This workshop is designed to kick start your journey, by helping you set up a local Kubernetes lab environment to practice and prepare for the exams. </p> <p>I won't cover every single certification exams topic in depth, but I will provide some hands-on exercises and tips I think will be helpful based on my own experience from preparing for and passing the exams.</p>"},{"location":"#learning-objectives","title":"Learning objectives","text":"<p>By the end of this workshop, you'll be able to:</p> <ul> <li>Set up a local multi-node Kubernetes lab environment using VMware Desktop Hypervisor </li> <li>Install and configure Ubuntu server with proper networking for Kubernetes nodes</li> <li>Build a Kubernetes cluster from scratch using kubeadm with containerd runtime</li> <li>Configure essential cluster components including Cilium CNI, MetalLB, and Ingress-Nginx Controller</li> <li>Practice hands-on exercises commonly found on Kubernetes exams (CKAD, CKA, CKS)</li> <li>Master effective exam techniques, including proper documentation navigation and time management strategies</li> <li>Gain practical troubleshooting experience using tools like vim, sed, systemctl, and journalctl</li> </ul> <p>Note</p> <p>You have other options for running a local Kubernetes lab environment, such as using Minikube, Kind, or K3s. But I chose to bootstrap a cluster with kubeadm on Ubuntu servers because it provides an environment that closely resembles a production cluster.</p>"},{"location":"#pre-requisites","title":"Pre-requisites","text":"<p>Most of the exercises in this workshop will be completed within a virtual machine. But you should also have the following tools also installed on your local machine to complete some of the exercises in this workshop:</p> <ul> <li>POSIX-compliant shell (bash, zsh, etc.)</li> <li>Docker Desktop</li> <li>Node.js</li> <li>Trivy</li> </ul>"},{"location":"#additional-resources","title":"Additional resources","text":"<p>There are many resources available to help you prepare for the Kubernetes certification exams. Some of the most useful resources include:</p> <ul> <li>CNCF Kubernetes Certification Programs - Official certification information</li> <li>Kubernetes Documentation - The comprehensive reference you'll use during exams</li> <li>Killer.sh - Simulator for CKA, CKAD, and CKS exams (included with exam purchase)</li> <li>Learn Kubernetes Basics - Tutorials for beginners</li> <li>Kubernetes the Hard Way - Deep-dive guide to manual cluster setup</li> </ul> <p>This workshop started as a series of blog posts on my blog site https://paulyu.dev so you can find the original content there as well as other cloud native content.</p> <p>Let's begin your journey to becoming a Kubestronaut \ud83d\ude80</p>"},{"location":"cka/","title":"Certified Kubernetes Administrator (CKA)","text":""},{"location":"cka/#overview","title":"Overview","text":"<p>After you get past the CKAD exam, you should have a solid understanding of fundamental Kubernetes concepts and be ready to move on to the next level. This exam, is just as intense as the CKAD exam but focuses on the administrative tasks that you would need to perform as a Kubernetes administrator. This includes things like setting up and managing clusters, managing workloads, taking backups of the etcd database, performing cluster upgrades, and more.</p> <p>Read through the Certified Kubernetes Administrator (CKA) page for Domain and Competency details and information on how to register for the exam.</p> <p>Some of the hands on activities that you should be comfortable with are:</p> <ul> <li>Upgrading a cluster</li> <li>Managing role-based access control (RBAC)</li> <li>Managing network policies</li> <li>Managing storage</li> <li>Managing cluster components</li> <li>etcd database backup and restore</li> <li>Configuring the kube-apiserver</li> <li>Troubleshooting a cluster</li> </ul> <p>To begin, SSH into the control node and switch to the root user.</p> <pre><code>sudo -i\n</code></pre> <p>Tip</p> <p>Many CKA tasks require elevated permissions to perform certain tasks especially when working with files and directories that are owned by the root user or using the crictl CLI tool. If you need to run kubectl commands as the root user, you'll need to export the KUBECONFIG environment variable by running the following command:</p> <pre><code>export KUBECONFIG=/etc/kubernetes/admin.conf\n</code></pre>"},{"location":"cka/#managing-core-components","title":"Managing core components","text":"<p>The core components of a Kubernetes cluster are:</p> <ul> <li>etcd</li> <li>kube-apiserver</li> <li>kube-controller-manager</li> <li>kube-scheduler</li> </ul> <p>When using kubeadm to bootstrap a cluster, these components are automatically deployed as static pods. Static pods are managed by the kubelet and are defined in manifest files located at <code>/etc/kubernetes/manifests</code> on the control plane node. The kubelet will automatically start and manage these pods.</p> <p>Run the following command to view the manifest files.</p> <pre><code>ll /etc/kubernetes/manifests\n</code></pre> <p>To make changes to any of these components, you'll need to edit the manifest file and the kubelet will automatically restart the pod with the new configuration.</p> <p>Tip</p> <p>When making changes to the manifest files, you should always make a backup of the original file before making any changes. This will allow you to revert back to the original configuration if something goes wrong.</p> <p>The status of the static pods can be checked by running the following command.</p> <pre><code>crictl ps\n</code></pre> <p>If all is well, you should see output similar to the following.</p> <pre><code>CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD\n3193217d296c0       f9818b4cd5fcb       8 hours ago         Running             frr-metrics               0                   c890c7a3a0205       metallb-speaker-v5b4s\nb18b948ca4484       f9818b4cd5fcb       8 hours ago         Running             reloader                  0                   c890c7a3a0205       metallb-speaker-v5b4s\n9d613f23e25d4       f9818b4cd5fcb       8 hours ago         Running             frr                       0                   c890c7a3a0205       metallb-speaker-v5b4s\n5e1922db4e666       f1e519d0f31b3       8 hours ago         Running             speaker                   0                   c890c7a3a0205       metallb-speaker-v5b4s\n6c3e8111d7b31       2f6c962e7b831       8 hours ago         Running             coredns                   0                   66036843831f6       coredns-7c65d6cfc9-cvjb8\n62813ada99f50       2f6c962e7b831       8 hours ago         Running             coredns                   0                   c4b818a26a66a       coredns-7c65d6cfc9-vldm9\n35463beb7567d       62788cf8a7b25       8 hours ago         Running             cilium-operator           0                   1b0127859d68d       cilium-operator-595959995b-qj6rh\n1290ffeb922f3       00ec0a2d78b34       8 hours ago         Running             cilium-envoy              0                   5de31704930bb       cilium-envoy-f2f8h\n89c01101f07bf       866c4ad7c1bbd       8 hours ago         Running             cilium-agent              0                   9d9cacb4d69ed       cilium-9fm7w\n8cbedd9d34e91       dc056e81c1f77       8 hours ago         Running             kube-proxy                0                   278ea959179c6       kube-proxy-wbfgj\nc8f1fa1747310       e0b799edb30ee       8 hours ago         Running             kube-scheduler            0                   caed0974f8975       kube-scheduler-control\n9efee977a5534       27e3830e14027       8 hours ago         Running             etcd                      0                   c3e78bddfb219       etcd-control\ne2184b59d426e       873e20495ccf3       8 hours ago         Running             kube-apiserver            0                   7ea6c543df23b       kube-apiserver-control\ne1c292d2aa6be       389ff6452ae41       8 hours ago         Running             kube-controller-manager   0                   0ca20c7dfab9f       kube-controller-manager-control\n</code></pre>"},{"location":"cka/#managing-kube-apiserver","title":"Managing kube-apiserver","text":"<p>The kube-apiserver is the control plane component that exposes the Kubernetes API. It is the front-end for the Kubernetes control plane. So securing the kube-apiserver is critical to the security of the entire cluster. Most of your tasks will involve making changes to the kube-apiserver configuration.</p> <p>The kube-apiserver is configured using a manifest file located at <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>.</p> <p>Run the following command to view the kube-apiserver static pod manifest file.</p> <pre><code>cat /etc/kubernetes/manifests/kube-apiserver.yaml\n</code></pre> <p>Tip</p> <p>There are a lot of flags that you can use to configure the kube-apiserver. Be sure to bookmark the kube-apiserver reference for a complete list and explanation of each flag.</p> <p>Whenever you are asked to make changes to the way the kube-apiserver is accessed or secured, this is the file you need to edit and the kubelet will automatically restart the kube-apiserver Pod with the new configuration.</p>"},{"location":"cka/#enable-validatingadmissionpolicy","title":"Enable ValidatingAdmissionPolicy","text":"<p>Run the following command to edit the kube-apiserver manifest to enable the ValidatingAdmissionPolicy admission plugin. It validates requests to the Kubernetes API before they are persisted to etcd. It can be used to enforce policies like ensuring that all Pods have resource requests and limits, or that all Pods have a certain label.</p> <pre><code>vim /etc/kubernetes/manifests/kube-apiserver.yaml\n</code></pre> <p>Locate the <code>--enable-admission-plugins</code> flag, add <code>ValidatingAdmissionPolicy</code> to the list of plugins, then save and exit the file.</p> <p>Note</p> <p>The list of plugins is comma separated.</p> <p>After making changes to the kube-apiserver manifest you should keep a watch on the static Pods to ensure the kube-apiserver restarts with the new configuration. It is very common to make a mistake and not realize it until you try to use kubectl and it doesn't work.</p> <p>Run the following command to watch the static Pods and ensure they are all running.</p> <pre><code>watch crictl ps\n</code></pre> <p>You will see the kube-apiserver be removed from the list of running Pods and then reappear with the new configuration.</p> <p>Tip</p> <p>If you don't see the kube-apiserver reappear, that probably means there was an error in the configuration file. So it is always a good idea to make a backup of the original file before making any changes.</p>"},{"location":"cka/#troubleshooting-kubelet","title":"Troubleshooting kubelet","text":"<p>The kubelet runs as a process on each node in the cluster and is responsible for managing the Pods and containers. </p> <p>You might encounter a situation where the components aren't running. In this case, you should check the status of the kubelet process by running the following command.</p> <pre><code>systemctl status kubelet\n</code></pre> <p>If you see <code>active (running)</code> highlighted in green, then the kubelet is running. If not, you'll need to start the kubelet by running the following command.</p> <pre><code>systemctl start kubelet\n</code></pre> <p>If the kubelet is running but you suspect some components aren't running. You can check the status of the components by running the following command.</p> <pre><code>crictl ps\n</code></pre> <p>You can see in the output that a few components aren't running, like the kube-apiserver and kube-controller-manager. This should lead you to check the manifests for these components to see what is going on or check the logs of the kubelet to see if there are any errors.</p> <pre><code>CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD\n88639aaca1dd1       d48f992a22722       2 days ago          Running             kube-scheduler            18                  ec844d5e5a039       kube-scheduler-control\n2c7467d60a7a6       014faa467e297       3 days ago          Running             etcd                      7                   f5c73fde48d08       etcd-control\ncdfa1217a1a25       8e97cdb19e7cc       3 days ago          Running             kube-controller-manager   17                  f78f21be20d53       kube-controller-manager-control\n0930536d04d9a       528878d09e0ce       4 weeks ago         Running             cilium-envoy              6                   077c262fdc09c       cilium-envoy-9r7dk\nc6dcda213517c       2351f570ed0ea       4 weeks ago         Running             kube-proxy                6                   b4fbd18b3b311       kube-proxy-ffd5z\n</code></pre> <p>To help troubleshoot the kubelet, you can check the logs by running the following command.</p> <pre><code>journalctl -u kubelet\n</code></pre> <p>If you need to check to see how the kubelet is configured, you can run the following command.</p> <pre><code>ps aux | grep kubelet\n</code></pre> <p>Note</p> <p>This assumes the kubelet is running as a process on the node.</p> <p>The text will be a bit jumbled, but you should be able to see the flags that are being passed to the kubelet process.</p> <p>For example, you might see something like this.</p> <pre><code>root        9848  1.0  2.5 2044616 100760 ?      Ssl  13:37   0:37 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.10\n</code></pre> <p>This shows that the kubelet is using a configuration files located at <code>/etc/kubernetes/kubelet.conf</code> and <code>/var/lib/kubelet/config.yaml</code> which can also be helpful when troubleshooting.</p>"},{"location":"cka/#backing-up-and-restoring-etcd","title":"Backing up and restoring etcd","text":"<p>The etcd database is a key component of a Kubernetes cluster. It stores the state of the cluster and is used by the control plane components to coordinate and manage the cluster. Backing up the etcd database is critical to ensure that you can recover from any data loss or corruption.</p>"},{"location":"cka/#take-a-snapshot-of-etcd","title":"Take a snapshot of etcd","text":"<p>The etcdctl is the tool you'll use to manage the etcd database. It is installed on the control plane node but needs sudo access to run.</p> <p>Now run the following command to see the available commands.</p> <pre><code>etcdctl --help # or -h\n</code></pre> <p>Warning</p> <p>Make sure you've switched to the root user with <code>sudo -i</code> before running the above command.</p> <p>You will see there is a command to take a snapshot of the etcd database. Let's see what the command needs.</p> <pre><code>etcdctl snapshot save -h\n</code></pre> <p>The command needs a path to save the snapshot to and accepts a few flags to configure how the client connects to the etcd server.</p> <p>With kubeadm, etcd is run as a static pod on the control plane node and defined in a manifest file located at <code>/etc/kubernetes/manifests/etcd.yaml</code>. This file will give you everything you need to connect to the etcd server.</p> <p>You can view the manifest file by running the following command.</p> <pre><code>cat /etc/kubernetes/manifests/etcd.yaml\n</code></pre> <p>It's a fairly long file, but locate the <code>- command</code> section and you'll see the following:</p> <pre><code>- command:\n    - etcd\n    - --advertise-client-urls=https://172.16.25.132:2379\n    - --cert-file=/etc/kubernetes/pki/etcd/server.crt           # need this\n    - --client-cert-auth=true\n    - --data-dir=/var/lib/etcd\n    - --experimental-initial-corrupt-check=true\n    - --experimental-watch-progress-notify-interval=5s\n    - --initial-advertise-peer-urls=https://172.16.25.132:2380\n    - --initial-cluster=control=https://172.16.25.132:2380\n    - --key-file=/etc/kubernetes/pki/etcd/server.key            # need this\n    - --listen-client-urls=https://127.0.0.1:2379,https://172.16.25.132:2379\n    - --listen-metrics-urls=http://127.0.0.1:2381\n    - --listen-peer-urls=https://172.16.25.132:2380\n    - --name=control\n    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt\n    - --peer-client-cert-auth=true\n    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key\n    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n    - --snapshot-count=10000\n    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt         # need this\n</code></pre> <p>Based on what we see here, it looks like we can connect to the etcd server using the same certificates that are used by the Pod. The <code>--cert-file</code> and <code>--key-file</code> flags specify the client certificate and key to use for authentication. The <code>--trusted-ca-file</code> flag specifies the CA certificate to use for verifying the server's certificate.</p> <p>Using the certificates that the etcd pod uses, run the following command to take a snapshot of the etcd database and save it to <code>/tmp/etcd-backup.db</code>.</p> <pre><code>etcdctl snapshot save /tmp/etcd-backup.db \\\n--cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n--cert=/etc/kubernetes/pki/etcd/server.crt \\\n--key=/etc/kubernetes/pki/etcd/server.key\n</code></pre>"},{"location":"cka/#restore-etcd-from-a-snapshot","title":"Restore etcd from a snapshot","text":"<p>With the snapshot saved, let's see how to restore the etcd database from a snapshot.</p> <p>First thing you need to do is stop the etcd Pod. You can do this by simply moving the etcd manifest file to a different location. The kubelet will see that the manifest file is no longer there and stop the Pod.</p> <pre><code>cd /etc/kubernetes/manifests\n\n# move all manifest files to parent directory\nmv * .. \n</code></pre> <p>Make sure the control plane components aren't running.</p> <pre><code>watch crictl ps\n</code></pre> <p>Once you see that the etcd Pod isn't running, press <code>Ctrl+C</code> to exit the watch command and proceed to restore the etcd database from the snapshot. </p> <p>Let's run help on the <code>etcdctl snapshot restore</code> command to see what it needs.</p> <pre><code>etcdctl snapshot restore -h\n</code></pre> <p>You will see that the command needs a path to the snapshot file, a path to the data directory which is where the etcd database is stored, and a few flags to configure how the client connects to the etcd server, like the <code>etcdctl snapshot save</code> command.</p> <p>The <code>--data-dir</code> flag is the path to the data directory where the etcd database is stored. This is usually <code>/var/lib/etcd</code> but you can confirm this by looking at the <code>- command</code> section of the etcd static pod manifest file.</p> <p>When you perform a restore, you can specify a different data directory to restore to. This is useful if you want to keep the original data directory intact.</p> <p>Run the following command to restore the etcd database from the snapshot.</p> <pre><code>etcdctl snapshot restore /tmp/etcd-backup.db \\\n--data-dir=/var/lib/etcd-from-backup \\\n--cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n--cert=/etc/kubernetes/pki/etcd/server.crt \\\n--key=/etc/kubernetes/pki/etcd/server.key\n</code></pre> <p>Now that you've restored the etcd database to a different directory, you'll need to update the etcd static pod manifest file to point to the new data directory.</p> <p>Make a backup of the original etcd static pod manifest file.</p> <pre><code>cp ../etcd.yaml etcd.yaml.bak\n</code></pre> <p>Use Vim to edit the etcd static pod manifest file.</p> <pre><code>vim ../etcd.yaml\n</code></pre> <p>Locate the <code>- hostPath</code> spec under <code>volumes</code> for <code>etcd-data</code> and update the path to the new data directory.</p> <pre><code>- hostPath:\n    path: /var/lib/etcd-from-backup\n    type: DirectoryOrCreate\n  name: etcd-data\n</code></pre> <p>Save and exit the file.</p> <p>Move the static pod manifest files back to the <code>/etc/kubernetes/manifests</code> directory.</p> <pre><code># move all yaml files back\nmv ../*.yaml .\n</code></pre> <p>Run the following command and wait for the etcd Pod to start.</p> <pre><code>watch crictl ps\n</code></pre> <p>Reference: https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster</p>"},{"location":"cka/#securing-a-cluster","title":"Securing a cluster","text":"<p>We'll get to security a bit more in the next section but one component of cluster security is RBAC and ServiceAccounts. In Kubernetes, users aren't stored as API objects but are authenticated through various mechanisms (certificates, tokens, OIDC, etc.). Inside the cluster, workloads use Service Accounts for identity, which are Kubernetes API objects that can be bound to roles which define what actions can be performed on resources.</p> <p>When you manage RBAC in Kubernetes, you define a Role or ClusterRole which is a set of permissions. You then create a RoleBinding or ClusterRoleBinding which binds the Role or ClusterRole to a Service Account, user, or group. The difference between a Role and a ClusterRole is that a Role is namespaced and a ClusterRole isn't. Likewise a RoleBinding is namespaced and a ClusterRoleBinding isn't.</p> <p>Note</p> <p>Roles can only be combined with RoleBindings but a ClusterRole can be used with both RoleBindings and ClusterRoleBindings making it more flexible across namespaces.</p>"},{"location":"cka/#granting-rbac-to-serviceaccounts","title":"Granting RBAC to ServiceAccounts","text":"<p>Run the following command to create a new Service Account.</p> <pre><code>kubectl create serviceaccount my-service-account\n</code></pre> <p>Run the following command to create a new Role.</p> <pre><code>kubectl create role my-role --verb=get --verb=list --resource=pods\n</code></pre> <p>Run the following command to bind the Role to the Service Account.</p> <pre><code>kubectl create rolebinding my-role-binding --role=my-role --serviceaccount=default:my-service-account\n</code></pre> <p>Run the following command to verify the service account can only list pods.</p> <pre><code>kubectl auth can-i list pods --as=system:serviceaccount:default:my-service-account\n</code></pre> <p>You should see <code>yes</code> as the output.</p> <p>Run the following command to verify the service account cannot create pods.</p> <pre><code>kubectl auth can-i create pods --as=system:serviceaccount:default:my-service-account\n</code></pre> <p>You should see <code>no</code> as the output.</p> <p>Reference: https://kubernetes.io/docs/reference/access-authn-authz/rbac/</p>"},{"location":"cka/#upgrading-a-cluster","title":"Upgrading a cluster","text":"<p>Upgrading a cluster is a critical task that needs to be done carefully to avoid any downtime. The upgrade process involves upgrading the control plane components and then upgrading the worker nodes. It's a tedious and time consuming task on the exam, but worth a lot of points and can be done quickly with practice.</p>"},{"location":"cka/#control-plane","title":"Control plane","text":"<p>Run the command to get the list of nodes and the Kubernetes version they are running.</p> <pre><code>kubectl get nodes\n</code></pre> <p>Warning</p> <p>When running as the root user, you'll need to run the following command to export the KUBECONFIG environment variable to run kubectl commands.</p> <pre><code>export KUBECONFIG=/etc/kubernetes/admin.conf\n</code></pre>"},{"location":"cka/#drain-node","title":"Drain node","text":"<p>Before upgrading the control plane, you should drain the control node to prevent any new Pods from being scheduled on it.</p> <pre><code>kubectl drain control --ignore-daemonsets\n</code></pre>"},{"location":"cka/#package-repository-updates","title":"Package repository updates","text":"<p>Check to see what version of kubeadm and kubelet you are running.</p> <pre><code>kubeadm version\nkubelet --version\n</code></pre> <p>Both kubeadm and kubelet should be running v1.31.6. Change the package repository to point to the v1.32.x packages.</p> <pre><code>sed -i 's/v1.31/v1.32/g' /etc/apt/sources.list.d/kubernetes.list\n</code></pre> <p>Update the package list.</p> <pre><code>apt update\n</code></pre> <p>Reference: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/change-package-repository/#verifying-if-the-kubernetes-package-repositories-are-used</p> <p>Check to see what versions of the Kubernetes packages you can upgrade to.</p> <pre><code>apt list --upgradable | grep kube\n</code></pre> <p>The packages were held back when you installed them to prevent automatic upgrades. Unhold the packages by running the following command.</p> <pre><code>apt-mark unhold kubeadm\n</code></pre> <p>Upgrade the kubeadm package to v1.32.2.</p> <pre><code>apt install -y kubeadm=1.32.2-1.1\n</code></pre> <p>Hold the kubeadm package again to prevent inadvertent upgrades.</p> <pre><code>apt-mark hold kubeadm\n</code></pre>"},{"location":"cka/#kubeadm-upgrade","title":"kubeadm upgrade","text":"<p>Run the following command to check if the upgrade is possible.</p> <pre><code>kubeadm upgrade plan\n</code></pre> <p>Note</p> <p>If you see any preflight errors, you may be able to simply re-run the command as they are often transient and resolve themselves.</p> <p>If the upgrade check went well, you'll see the following output.</p> <pre><code>[preflight] Running pre-flight checks.\n[upgrade/config] Reading configuration from the \"kubeadm-config\" ConfigMap in namespace \"kube-system\"...\n[upgrade/config] Use 'kubeadm init phase upload-config --config your-config.yaml' to re-upload it.\n[upgrade] Running cluster health checks\n[upgrade] Fetching available versions to upgrade to\n[upgrade/versions] Cluster version: 1.31.6\n[upgrade/versions] kubeadm version: v1.32.2\n[upgrade/versions] Target version: v1.32.2\n[upgrade/versions] Latest version in the v1.31 series: v1.31.6\n\nComponents that must be upgraded manually after you've upgraded the control plane with 'kubeadm upgrade apply':\nCOMPONENT   NODE       CURRENT   TARGET\nkubelet     worker-1   v1.31.6   v1.32.2\nkubelet     worker-2   v1.31.6   v1.32.2\nkubelet     control    v1.32.2   v1.32.2\n\nUpgrade to the latest stable version:\n\nCOMPONENT                 NODE      CURRENT    TARGET\nkube-apiserver            control   v1.31.6    v1.32.2\nkube-controller-manager   control   v1.31.6    v1.32.2\nkube-scheduler            control   v1.31.6    v1.32.2\nkube-proxy                          1.31.6     v1.32.2\nCoreDNS                             v1.11.3    v1.11.3\netcd                      control   3.5.15-0   3.5.16-0\n\nYou can now apply the upgrade by executing the following command:\n\n    kubeadm upgrade apply v1.32.2\n\n_____________________________________________________________________\n\n\nThe table below shows the current state of component configs as understood by this version of kubeadm.\nConfigs that have a \"yes\" mark in the \"MANUAL UPGRADE REQUIRED\" column require manual config upgrade or\nresetting to kubeadm defaults before a successful upgrade can be performed. The version to manually\nupgrade to is denoted in the \"PREFERRED VERSION\" column.\n\nAPI GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED\nkubeproxy.config.k8s.io   v1alpha1          v1alpha1            no\nkubelet.config.k8s.io     v1beta1           v1beta1             no\n_____________________________________________________________________\n</code></pre> <p>Upgrade the control plane components by running the following command.</p> <pre><code>kubeadm upgrade apply v1.32.2 --v=5\n</code></pre> <p>When asked if you are sure you want to proceed, type <code>y</code> and press <code>Enter</code>.</p> <p>Warning</p> <p>The upgrade process can take several minutes to complete.</p>"},{"location":"cka/#kubelet-and-kubectl-upgrades","title":"kubelet and kubectl upgrades","text":"<p>When the control plane upgrade is complete, you can move on to upgrading the kubelet and kubectl. Unhold the kubelet and kubectl packages so they can be upgraded.</p> <pre><code>apt-mark unhold kubelet kubectl\n</code></pre> <p>Upgrade the kubelet and kubectl packages to v1.32.2.</p> <pre><code>apt install kubelet=1.32.2-1.1 kubectl=1.32.2-1.1\n</code></pre> <p>Hold the kubelet and kubectl packages again to prevent inadvertent upgrades.</p> <pre><code>apt-mark hold kubelet kubectl\n</code></pre>"},{"location":"cka/#uncordon-node","title":"Uncordon node","text":"<p>With the control plane upgraded, you can uncordon the control node to allow new Pods to be scheduled on it again.</p> <pre><code>kubectl uncordon control\n</code></pre> <p>Note</p> <p>You might need to wait a minute or two for the control plane components to come back up since the kubelet will need to restart and eventually restart the static pods.</p>"},{"location":"cka/#verify-upgrade","title":"Verify upgrade","text":"<p>Run the following command to verify the control node is upgraded to v1.32.2.</p> <pre><code>kubectl get nodes\n</code></pre> <p>Your output should look like this.</p> <pre><code>NAME       STATUS   ROLES           AGE   VERSION\ncontrol    Ready    control-plane   15h   v1.32.2  # upgraded\nworker-1   Ready    &lt;none&gt;          15h   v1.31.6\nworker-2   Ready    &lt;none&gt;          15h   v1.31.6\n</code></pre> <p>You can see the control node is now running v1.32.2 and the worker nodes are still running v1.31.6. You need to upgrade the worker nodes next.</p>"},{"location":"cka/#worker-nodes","title":"Worker nodes","text":""},{"location":"cka/#drain-node_1","title":"Drain node","text":"<p>Before upgrading the worker nodes, you should drain the worker nodes to prevent any new Pods from being scheduled on them.</p> <pre><code>kubectl drain worker-1 --ignore-daemonsets\n</code></pre> <p>Warning</p> <p>Make sure you are back on the control node before running the above command.</p>"},{"location":"cka/#package-repository-updates_1","title":"Package repository updates","text":"<p>SSH into the worker node and switch to the root user.</p> <pre><code>sudo -i\n</code></pre> <p>Update the package repository to point to the v1.32.x packages.</p> <pre><code>sed -i 's/v1.31/v1.32/g' /etc/apt/sources.list.d/kubernetes.list\n</code></pre> <p>Update the package list.</p> <pre><code>apt update\n</code></pre> <p>Unhold the kubeadm package so it can be upgraded.</p> <pre><code>apt-mark unhold kubeadm\n</code></pre> <p>Upgrade the kubeadm package to v1.32.2.</p> <pre><code>apt install kubeadm=1.32.2-1.1\n</code></pre> <p>Hold the kubeadm package to prevent automatic upgrades again.</p> <pre><code>apt-mark hold kubeadm\n</code></pre>"},{"location":"cka/#kubeadm-upgrade_1","title":"kubeadm upgrade","text":"<p>Upgrade the worker node by running the following command.</p> <pre><code>kubeadm upgrade node --v=5\n</code></pre>"},{"location":"cka/#kubelet-and-kubectl-upgrades_1","title":"kubelet and kubectl upgrades","text":"<p>Unhold the kubelet and kubectl packages so they can be upgraded.</p> <pre><code>apt-mark unhold kubectl kubelet\n</code></pre> <p>Upgrade the kubelet and kubectl packages to v1.32.2.</p> <pre><code>apt install kubelet=1.32.2-1.1 kubectl=1.32.2-1.1\n</code></pre> <p>Hold the kubelet and kubectl packages to prevent automatic upgrades again.</p> <pre><code>apt-mark hold kubectl kubelet\n</code></pre>"},{"location":"cka/#restart-kubelet","title":"Restart kubelet","text":"<p>Restart the kubelet service.</p> <pre><code>service kubelet restart\n</code></pre> <p>Wait a minute or two and check to make sure the kubelet service is running.</p> <pre><code>service kubelet status\n</code></pre>"},{"location":"cka/#verify-upgrade_1","title":"Verify upgrade","text":"<p>Back in the control node, run the following command to see if the worker node is ready.</p> <pre><code>kubectl get nodes\n</code></pre> <p>Warning</p> <p>Make sure you are back on the control node before running the above command.</p> <p>Now you should see a worker node running v1.32.2.</p> <pre><code>NAME       STATUS                     ROLES           AGE   VERSION\ncontrol    Ready                      control-plane   15h   v1.32.2 # upgraded\nworker-1   Ready,SchedulingDisabled   &lt;none&gt;          15h   v1.32.2 # upgraded\nworker-2   Ready                      &lt;none&gt;          15h   v1.31.6\n</code></pre>"},{"location":"cka/#uncordon-node_1","title":"Uncordon node","text":"<p>Uncordon the worker node to allow new Pods to be scheduled on it.</p> <pre><code>kubectl uncordon worker-1\n</code></pre> <p>Danger</p> <p>Drain the next worker node and repeat the above steps for the remaining worker nodes.</p> <p>Reference: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/</p>"},{"location":"cka/#additional-resources","title":"Additional Resources","text":"<ul> <li>CKA Exam Curriculum</li> <li>CKA Certification Learning Path</li> </ul>"},{"location":"ckad/","title":"Certified Kubernetes Application Developer (CKAD)","text":""},{"location":"ckad/#overview","title":"Overview","text":"<p>Once you get past the KCNA and KCSA exams, you should have your sights set on the CKAD exam. This was once considered an entry-level exam but due to its intense hands-on nature, it's better classified as intermediate. Despite this, it makes an excellent starting point for those looking to dive deeper into Kubernetes.</p> <p>Read through the Certified Kubernetes Application Developer (CKAD) page for Domain and Competency details and information on how to register for the exam.</p> <p>Some of the hands on activities that you should be comfortable with are:</p> <ul> <li>Deploying applications and choosing the right workload resource type for the job (i.e., Deployment, StatefulSet, DaemonSet, Job, CronJob)</li> <li>Configuring applications using ConfigMaps and Secrets</li> <li>Creating and managing Persistent Volumes and Persistent Volume Claims</li> <li>Using ServiceAccounts to give applications the right permissions</li> <li>Using network policies to restrict traffic to applications</li> <li>Using Ingress resources to expose applications to the outside world</li> </ul>"},{"location":"ckad/#deploying-applications","title":"Deploying Applications","text":"<p>A Pod is the smallest deployable unit in Kubernetes. It is a logical host for one or more containers which runs your application.</p> <p>You would not typically create Pods directly, but rather use a workload resource to manage the pods for you. There are several different types of workload resources in Kubernetes, each with its own use case and knowing when to use each is important. The most common types of workload resources are:</p> <ul> <li>Deployment: A deployment is a declarative way to manage a set of replicas of a pod. It is the most common way to deploy applications in Kubernetes.</li> <li>ReplicaSet: A replica set is a workload resource that is used to ensure that a specified number of pod replicas are running at any given time. It is used to manage the scaling of applications.</li> <li>StatefulSet: A stateful set is a workload resource that is used to manage stateful applications. It is used for applications that require stable, unique network identifiers and stable storage.</li> <li>DaemonSet: A daemon set is a workload resource that ensures that a copy of a pod is running on all or some nodes in the cluster. It is used for applications that need to run on every node in the cluster, such as logging or monitoring agents.</li> <li>Job: A job is a workload resource that is used to run a batch job. It is used for applications that need to run to completion, such as data processing jobs.</li> <li>CronJob: A cron job is a workload resource that is used to run a batch job on a schedule. It is used for applications that need to run on a schedule, such as backups or report generation.</li> </ul> <p>The resources that you request are reconciled by various controllers in the Kubernetes control plane. For example, when you create a Deployment, the Deployment controller will create a ReplicaSet and the ReplicaSet controller will create the Pods. When you submit a resource through the Kubernetes API, the desired state is stored in etcd and controllers are responsible for ensuring that the actual state matches the desired state. This is known as the reconciliation loop.</p> <p>Info</p> <p>Kubernetes also supports custom controllers through its extension patterns which allow you to extend the functionality of Kubernetes.</p>"},{"location":"ckad/#use-kubectl-to-generate-yaml","title":"Use kubectl to generate YAML","text":"<p>In the KCNA section of this workshop, you used kubectl imperative commands to deploy applications. That is a good way to quickly get started but it isn't the best way to deploy applications in production. In production, you should be using declarative configuration files in the form of YAML manifests to deploy applications. This allows you to version control application deployments and makes it easier to manage changes over time.</p> <p>Getting started with YAML might seem daunting at first, but it can be made easy if you leverage a feature of kubectl and have it generate the YAML for you. </p> <p>Run the following command to create a namespace.</p> <pre><code>kubectl create namespace n654\n</code></pre> <p>Let's deploy the application that you packaged up in the KCNA section of this workshop.</p> <p>Run the following command to create a deployment, but this time use the <code>--dry-run=client</code> and <code>-o yaml</code> flags to generate the YAML for you and redirect the output to a file called myapp2.yaml.</p> <pre><code>kubectl create deployment myapp2 \\\n--namespace n654 \\\n--image=&lt;PASTE_IN_YOUR_CONTAINER_IMAGE_NAME&gt; \\\n--replicas=3 \\\n--port=3000 \\\n--dry-run=client \\\n-o yaml &gt; myapp2.yaml\n</code></pre> <p>Warning</p> <p>This assumes you have a container image that you built in the KCNA section of this workshop.</p> <p>View the myapp2.yaml file.</p> <pre><code>cat myapp2.yaml\n</code></pre> <p>In the output, you'll see that the entire Deployment manifest has been generated for you. If needed, you can edit the file to make any changes that you want and then apply it.</p> <p>Run the following command to apply the manifest.</p> <pre><code>kubectl apply --namespace n654 -f myapp2.yaml\n</code></pre> <p>The application should now be deployed. Run the following command to get the Pods.</p> <pre><code>kubectl get pods --namespace n654\n</code></pre> <p>Why stop there? We can add the Service and Ingress resources to the same file too.</p> <p>In YAML manifests, you can have multiple resources in the same file as long as they are separated by three dashes. So let's add that then append the service YAML to the file.</p> <pre><code>echo \"---\" &gt;&gt; myapp2.yaml\n</code></pre> <p>Then run the following command to generate the Service YAML and append it to the file.</p> <pre><code>kubectl expose deployment myapp2 \\\n--namespace n654 \\\n--port=80 \\\n--target-port=3000 \\\n--dry-run=client \\\n-o yaml &gt;&gt; myapp2.yaml\n</code></pre> <p>Do the same for creating an Ingress for the application to expose it externally using a load balancer.</p> <pre><code>echo \"---\" &gt;&gt; myapp2.yaml\n</code></pre> <p>Then run the following command to generate the ingress YAML and append it to the file.</p> <pre><code>kubectl create ingress myapp2 \\\n--namespace n654 \\\n--class=nginx \\\n--rule=\"myapp2.example.com/*=myapp2:80\" \\\n--dry-run=client \\\n-o yaml &gt;&gt; myapp2.yaml\n</code></pre> <p>Now, view the myapp2.yaml file and notice how it has all three resources in it.</p> <pre><code>cat myapp2.yaml\n</code></pre> <p>Finally, apply the manifest.</p> <pre><code>kubectl apply --namespace n654 -f myapp2.yaml\n</code></pre> <p>Tip</p> <p>You can use the <code>--dry-run=client</code> and <code>-o yaml</code> flags to generate the YAML for any resource type. Throughout the exam, you should leverage this technique as much as possible to save time. It is easier to use kubectl to generate most of the YAML for you than it is to write it from scratch.</p> <p>If all went well, you should be able to access the application from your host machine using the following URL.</p> <pre><code>curl http://control -H \"Host: myapp2.example.com\"\n</code></pre>"},{"location":"ckad/#use-kubectl-for-reference","title":"Use kubectl for reference","text":"<p>kubectl is also a great tool for reference. You can use it to get information about resources in the cluster. </p> <p>To see all the available resources in the cluster, run the following command.</p> <pre><code>kubectl api-resources\n</code></pre> <p>The output will show you all the resources that are available in the cluster. You will see the short name, full name, and the namespaced status of each resource. The short name in particular is useful for quickly referencing resources in kubectl commands. Remember, time is of the essence so you save a few seconds by using the short name. For example, you can use <code>po</code> instead of <code>pods</code>. Combine this with command aliases and you can save even more time.</p> <p>You can run something like this to get information about an Ingress resource.</p> <pre><code>k get ing -n n654\n</code></pre> <p>You can also use the <code>explain</code> command to get detailed information about a resource spec. For example, if you want to know more about the Ingress resource, you can run the following command.</p> <pre><code>kubectl explain ingress\n</code></pre> <p>To dig deeper into the spec, you can run the following command.</p> <pre><code>kubectl explain ingress.spec\n</code></pre> <p>You can also use the <code>--recursive</code> flag to get information about nested fields, but it won't give you the full details of each field.</p> <pre><code>kubectl explain ingress.spec --recursive\n</code></pre> <p>The <code>--help</code> flag is also incredibly useful. You can use it to get information about the flags that are available for a command. It will even give you examples of how to use the command.</p> <p>Run this command and see what you get.</p> <pre><code>kubectl create configmap --help\n</code></pre> <p>Tip</p> <p>kubectl is your friend. Use it to get information you need to get a task done. It is faster than looking up the documentation.</p>"},{"location":"ckad/#configuring-applications","title":"Configuring Applications","text":"<p>All applications need to be configured in some way. In Kubernetes, there are two main ways to configure applications: using ConfigMaps and Secrets.</p> <p>ConfigMaps are used to store non-sensitive configuration data in key-value pairs. They can be used to store configuration files, command-line arguments, environment variables, and more. ConfigMaps can be mounted as volumes or exposed as environment variables in pods.</p> <p>Secrets are used to store sensitive information, such as passwords, OAuth tokens, and SSH keys. Secrets stored in etcd and are base64 encoded --not encrypted; which is a key consideration when using them. Anyone with access to the etcd database can read the secrets. Therefore, it is important to use encryption at rest for etcd data.</p> <p>Applications may also need to be configured to use persistent storage. In Kubernetes, persistent storage is managed using Persistent Volumes (PV) and Persistent Volume Claims (PVC). A PV is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. A PVC is a request for storage by a user.</p>"},{"location":"ckad/#using-pvs-and-pvcs","title":"Using PVs and PVCs","text":"<p>PVs and PVCs are used to manage persistent storage usage in Kubernetes. To use persistent storage, you need to create a PV to represent a chunk of storage and a PVC to request the storage --an allocation. There are several different types of PVs, including hostPath, NFS, iSCSI, and cloud provider-specific PVs. </p> <p>Note</p> <p>Since we are working with a local Kubernetes cluster and all we have are local disks, we will use the hostPath type of PV. The hostPath type of PV uses a file or directory on the host to store the data. This is useful for testing and development purposes but isn't recommended for production use.</p> <p>Run the following command to create a PV on the host's /tmp directory. We don't have much storage space to work with so we'll only carve out 20Mi of storage.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n name: my-pv\nspec:\n capacity:\n  storage: 20Mi\n accessModes:\n   - ReadWriteOnce\n hostPath:\n  path: \"/tmp\"\nEOF\n</code></pre> <p>Note</p> <p>The PV is a cluster resource that represents a piece of storage in the cluster so a namespace isn't needed.</p> <p>Run the following command to view the PV. You should see that the PV is available. This is because the PV hasn't been claimed by a PVC yet.</p> <pre><code>kubectl get pv -n n239\n</code></pre> <p>Now create a PVC that requests the 20Mi of storage.</p> <pre><code>kubectl create ns n239\nkubectl apply -n n239 -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 20Mi\nEOF\n</code></pre> <p>If you run the command to view the PV again, you'll see that the PV is bound to the PVC.</p> <pre><code>kubectl get pv -n n239\n</code></pre> <p>Additionally, if following command to view the PVC, you'll see that the PVC is linked to the PV.</p> <pre><code>kubectl get pvc -n n239\n</code></pre> <p>PVCs are then used in workload resources and mounted as volumes in the Pod spec. Run the following command to create a Pod that uses the PVC.</p> <pre><code>kubectl apply -n n239 -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: my-pod\n    image: busybox\n    command: [\"sleep\", \"3600\"]\n    volumeMounts:                # Define the volume mounts for the container\n      - name: data               # Name of the volume\n        mountPath: /data         # Path to mount the volume in the container\n  volumes:                       # Define the volumes for the Pod\n    - name: data                 # Name of the volume\n      persistentVolumeClaim:     # Source type of the volume\n        claimName: my-pvc        # Name of the PVC\nEOF\n</code></pre> <p>If you run the following command, you should be able to view the contents of the mounted volume in the Pod.</p> <pre><code>kubectl exec -it my-pod -n n239 -- ls /data\n</code></pre> <p>Since we used the host's /tmp directory for the PV, the Pod now has access to the host's /tmp directory and all the files within it.</p> <p>Info</p> <p>This is a security risk and should be avoided in production. Another concern of using hostPath is that the data is limited to the host that the Pod was scheduled on. If the Pod is rescheduled to another host, the data will not be available.</p>"},{"location":"ckad/#using-configmaps","title":"Using ConfigMaps","text":"<p>There are a few ways to use ConfigMaps in Kubernetes. The most common way is to use them as environment variables in a pod. This allows you to pass configuration data to your application without hardcoding it into the application itself.</p> <p>In this example, you'll create a ConfigMap and use it to configure an application.</p> <p>Run the following command to create a temporary file to store plugin configuration for RabbitMQ.</p> <pre><code>cat &lt;&lt; EOF | tee /tmp/rabbitmq_enabled_plugins\n[rabbitmq_management,rabbitmq_prometheus,rabbitmq_amqp1_0].\nEOF\n</code></pre> <p>Create a namespace and a ConfigMap from the RabbitMQ plugins file.</p> <pre><code>kubectl create ns n754\nkubectl create configmap rabbitmq-enabled-plugins -n n754 --from-file=/tmp/rabbitmq_enabled_plugins\n</code></pre> <p>Next, run the following command to use the ConfigMap as a file mounted in the RabbitMQ Pod to enable the plugins.</p> <pre><code>kubectl apply -n n754 -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: rabbitmq\nspec:\n  serviceName: rabbitmq\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rabbitmq\n  template:\n    metadata:\n      labels:\n        app: rabbitmq\n    spec:\n      nodeSelector:\n        \"kubernetes.io/os\": linux\n      containers:\n      - name: rabbitmq\n        image: rabbitmq:3.10-management-alpine\n        ports:\n        - containerPort: 5672\n          name: rabbitmq-amqp\n        - containerPort: 15672\n          name: rabbitmq-http\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: \"username\"\n        - name: RABBITMQ_DEFAULT_PASS\n          value: \"password\"\n        resources:\n          requests:\n            cpu: 10m\n            memory: 128Mi\n          limits:\n            cpu: 250m\n            memory: 256Mi\n        volumeMounts:                                 # Define the volume mounts for the container\n        - name: rabbitmq-enabled-plugins              # Name of the volume\n          mountPath: /etc/rabbitmq/enabled_plugins    # Path to mount the volume in the container\n          subPath: enabled_plugins                    # Mount as a file in the container\n      volumes:                                        # Define the volume for the Pod\n      - name: rabbitmq-enabled-plugins                # Name of the volume\n        configMap:                                    # Source type of the volume\n          name: rabbitmq-enabled-plugins              # Name of the ConfigMap\n          items:                                      # Specify the items to mount\n          - key: rabbitmq_enabled_plugins             # Name of the key in the ConfigMap\n            path: enabled_plugins                     # Name of the file in the Pod\nEOF\n</code></pre> <p>Based on the comments in the YAML, you can see that we're mounting the ConfigMap as a file in the container. The <code>subPath</code> field is used to specify the name of the file in the container. This allows us to mount a single key from the ConfigMap as a file in the container.</p> <p>If you run the following command, you should be able to view the contents of the plugins file in the RabbitMQ Pod.</p> <pre><code>kubectl exec -it rabbitmq-0 -n n754 -- cat /etc/rabbitmq/enabled_plugins | grep rabbitmq_amqp1_0\n</code></pre> <p>The other ways to use ConfigMaps is to mount them as environment variables, so be sure to check out ths guide on how to do that.</p> <p>Reference: https://kubernetes.io/docs/tutorials/configuration/updating-configuration-via-a-configmap/</p>"},{"location":"ckad/#using-secrets","title":"Using Secrets","text":"<p>Secrets are used to store \"sensitive\" information. I put that in quotes because secrets aren't really secret. They are base64 encoded and anyone with access to the etcd database can read them. In terms of usage, they are similar to ConfigMaps. You can use them as environment variables or mount them as volumes.</p> <p>If you noticed in the previous example, the username and password for RabbitMQ were hardcoded in the YAML. This isn't a good practice. Instead, you should use a Secret to store the username and password.</p> <p>Run the following command to create a temporary file to store the RabbitMQ username and password.</p> <pre><code>kubectl create secret generic rabbitmq-secret -n n754 --from-literal=RABBITMQ_DEFAULT_USER=username --from-literal=RABBITMQ_DEFAULT_PASSWORD=password\n</code></pre> <p>Run the following command to use the Secret as an environment variable in the RabbitMQ Pod to set the admin user and password, and expose the RabbitMQ management interface on port 15672.</p> <pre><code>kubectl apply -n n754 -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: rabbitmq\nspec:\n  serviceName: rabbitmq\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rabbitmq\n  template:\n    metadata:\n      labels:\n        app: rabbitmq\n    spec:\n      nodeSelector:\n        \"kubernetes.io/os\": linux\n      containers:\n      - name: rabbitmq\n        image: rabbitmq:3.10-management-alpine\n        ports:\n        - containerPort: 5672\n          name: rabbitmq-amqp\n        - containerPort: 15672\n          name: rabbitmq-http\n        envFrom:                       # Use envFrom to load all keys from the secret as environment variables\n        - secretRef:                   # Reference the secret\n            name: rabbitmq-secret      # Name of the secret\n        resources:\n          requests:\n            cpu: 10m\n            memory: 128Mi\n          limits:\n            cpu: 250m\n            memory: 256Mi\nEOF\n</code></pre> <p>If you run the following command, you'll see that the username and password are now being set as environment variables in the RabbitMQ Pod.</p> <pre><code>kubectl describe pod rabbitmq-0 -n n754 | grep \"Environment Variables from\" -A 1\n</code></pre>"},{"location":"ckad/#exposing-applications","title":"Exposing Applications","text":"<p>Understanding the different types of services is also important. There are four types of services in Kubernetes: ClusterIP, NodePort, LoadBalancer, and ExternalName. </p> <p>The most common type of service is ClusterIP, which exposes the service on a cluster-internal IP. This means that the service is only accessible from within the cluster. NodePort exposes the service on each node's IP at a static port. This means that the service is accessible from outside the cluster by requesting the node IP on the node port. LoadBalancer exposes the service externally using a cloud provider's load balancer, which ultimately gives you a public or private IP address. ExternalName maps the service to the contents of the externalName field (e.g., foo.bar.example.com), by returning a CNAME record with its value.</p> <p>It is also important to remember that services uses the selector to find the pods that it should route traffic to. This is done using labels. Labels are key-value pairs that are attached to resources in Kubernetes.</p>"},{"location":"ckad/#using-services","title":"Using Services","text":"<p>Run the following command to expose the RabbitMQ deployment that you just created.</p> <pre><code>kubectl apply -n n754 -f - &lt;&lt;EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: rabbitmq\n  labels:\n    app: rabbitmq\nspec:\n  ports:\n    - name: rabbitmq-amqp\n      port: 5672\n      targetPort: 5672\n    - name: rabbitmq-http\n      port: 15672\n      targetPort: 15672\n  selector:\n    app: rabbitmq      # The selector is used to find the pods that the service should route traffic to\nEOF\n</code></pre> <p>With the service created, you can access the nginx application using the service name. Run the following command to get the service IP.</p> <pre><code>kubectl run mycurl -n n754 --image=curlimages/curl -it --rm --restart=Never -- curl rabbitmq:15672\n</code></pre>"},{"location":"ckad/#using-ingress","title":"Using Ingress","text":"<p>The default service type is ClusterIP, which means that the service is only accessible from within the cluster. We demonstrated this with the curl command above with the curl pod running in the same cluster, you were able to access the RabbitMQ service using the service name. But services especially HTTP-based services are often meant to be accessed from outside the cluster. To do this, you can use an Ingress resource.</p> <p>Think of an Ingress as a reverse proxy that routes traffic to the appropriate service based on the request URL. It is most commonly used to route HTTP and HTTPS traffic, but it can also be used to route TCP and UDP traffic.</p> <p>Run the following command to create an Ingress resource for the RabbitMQ service.</p> <pre><code>kubectl apply -n n754 -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rabbitmq\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /      # Rewrite the URL to remove the path prefix\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: rabbitmq.example.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: rabbitmq\n            port:\n              number: 15672\n        path: /manage                                  # Request path to the RabbitMQ management interface\n        pathType: Prefix\n      - backend:\n          service:\n            name: rabbitmq\n            port:\n              number: 5672\n        path: /amqp                                    # Request path to the RabbitMQ AMQP interface\n        pathType: Prefix\nEOF\n</code></pre> <p>Note</p> <p>Notice that the Ingress resource is using path-based routing to route traffic to the appropriate service based on the request URL. The path /manage is used to route traffic to the RabbitMQ management interface, while the path /amqp is used to route traffic to the RabbitMQ AMQP interface. But these paths mean nothing to the actual RabbitMQ service so you're using the <code>nginx.ingress.kubernetes.io/rewrite-target</code> annotation to rewrite the URL to remove the path prefix before it is sent to the RabbitMQ service.</p> <p>Now you can access the RabbitMQ management interface from your host machine. Run the following command to access the RabbitMQ management interface.</p> <pre><code>curl http://control/manage -H \"Host: rabbitmq.example.com\"\n</code></pre> <p>Warning</p> <p>Make sure you run the command from your host machine and not from the control node.</p>"},{"location":"ckad/#securing-network-traffic","title":"Securing network traffic","text":"<p>Network policies are extremely important to understand for all of the hands-on exams. In the previous section, we were introduced to NetworkPolicy but it is worth revisiting it here as it is a major part of all the Kubernetes exams and tends to be a stumbling block for many test takers.</p> <p>In Kubernetes, there are two important behaviors to understand regarding network policies:</p> <ol> <li>By default, with no NetworkPolicies applied, all pods can communicate with all other pods</li> <li>Once you apply any NetworkPolicy with a podSelector to a namespace, the selected pods become isolated and only accept traffic explicitly allowed by NetworkPolicies.</li> </ol>"},{"location":"ckad/#deny-all-ingress-traffic","title":"Deny all ingress traffic","text":"<p>Run the following command to create a namespace and a deployment.</p> <pre><code>kubectl apply -n pets -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-all\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\nEOF\n</code></pre> <p>Warning</p> <p>This assumes you deployed the AKS Store Demo microservices app in the previous section section of this workshop. If you've not already, please go back and deploy it before you proceed.</p> <p>The deny-all policy you created earlier (with <code>podSelector: {}</code>) selects all pods in the namespace and defines no ingress rules, effectively blocking all incoming traffic to every pod.</p> <p>Let's test the network policies to confirm all ingress traffic is blocked.</p> <p>Run the following command to get the pod name of the store-front application.</p> <pre><code>POD_NAME=$(kubectl get pod --namespace pets -l app=store-front -o jsonpath='{.items[0].metadata.name}')\n</code></pre> <p>Connection from store-front to product-service should timeout.</p> <pre><code>kubectl exec -it $POD_NAME --namespace pets -- wget -qO- http://product-service:3002/health --timeout=1\n</code></pre> <p>Connection from store-front to order-service should also timeout.</p> <pre><code>kubectl exec -it $POD_NAME --namespace pets -- wget -qO- http://order-service:3000/health --timeout=1\n</code></pre> <p>Finally, get the pod name of the order-service and confirm the connection from order-service to RabbitMQ also times out.</p> <pre><code>POD_NAME=$(kubectl get pod --namespace pets -l app=order-service -o jsonpath='{.items[0].metadata.name}')\nkubectl exec -it $POD_NAME --namespace pets -- wget -qO- http://rabbitmq:15672/ --timeout=1\n</code></pre>"},{"location":"ckad/#configure-specific-traffic-patterns","title":"Configure specific traffic patterns","text":"<p>All traffic is blocked at the moment. Let's open up ingress traffic to the product-service from the store-front application. This is a common use case for network policies. You want to restrict access to a service to only allow traffic from specific applications.</p> <p>Run the following command to allow ingress on the product-service only from the store-front application.</p> <pre><code>kubectl apply -n pets -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-storefront-to-product-service\nspec:\n  podSelector:\n    matchLabels:\n      app: product-service\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: store-front\n  policyTypes:\n  - Ingress\nEOF\n</code></pre> <p>Similarly, the order-service should also only be accessible from the store-front application. So let's create a network policy for that as well.</p> <pre><code>kubectl apply -n pets -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-storefront-to-order-service\nspec:\n  podSelector:\n    matchLabels:\n      app: order-service\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: store-front\n  policyTypes:\n  - Ingress\nEOF\n</code></pre> <p>Finally, the RabbitMQ service should only be accessible from the order-service application. So let's create a network policy for that as well.</p> <pre><code>kubectl apply -n pets -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-order-service-to-rabbitmq\nspec:\n  podSelector:\n    matchLabels:\n      app: rabbitmq\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: order-service\n  policyTypes:\n  - Ingress\nEOF\n</code></pre> <p>Run the following command to get the pod name of the store-front application.</p> <pre><code>POD_NAME=$(kubectl get pod --namespace pets -l app=store-front -o jsonpath='{.items[0].metadata.name}')\n</code></pre> <p>Connection from store-front to product-service should now be successful.</p> <pre><code>kubectl exec -it $POD_NAME --namespace pets -- wget -qO- http://product-service:3002/health --timeout=1\n</code></pre> <p>Connection from store-front to order-service should now be successful as well.</p> <pre><code>kubectl exec -it $POD_NAME --namespace pets -- wget -qO- http://order-service:3000/health --timeout=1\n</code></pre> <p>Finally, get the pod name of the order-service and confirm the connection from order-service to RabbitMQ should now be successful.</p> <pre><code>POD_NAME=$(kubectl get pod --namespace pets -l app=order-service -o jsonpath='{.items[0].metadata.name}')\nkubectl exec -it $POD_NAME --namespace pets -- wget -qO- http://rabbitmq:15672/ --timeout=1\n</code></pre> <p>Network policies are a powerful tool for securing your Kubernetes cluster. It is important to understand how they work and how to use them effectively. It is also important to efficiently test your network policies to ensure that they are working as expected during the exam.</p> <p>For more NetworkPolicy practice, check out the NetworkPolicy Editor by Isovalent for an interactive experience.</p>"},{"location":"ckad/#additional-resources","title":"Additional Resources","text":"<p>There is a lot more to cover for the CKAD exam. Here are some additional resources to help you prepare:</p> <ul> <li>CKAD Exam Curriculum</li> <li>CKAD Certification Learning Path</li> <li>CKAD - free materials</li> </ul>"},{"location":"cks/","title":"Certified Kubernetes Security Specialist (CKS)","text":""},{"location":"cks/#overview","title":"Overview","text":"<p>The last exam in the path to become a Kubestronaut is the CKS exam. This exam is probably the most difficult exam to clear. You will be tested on your ability to not only secure Kubernetes clusters and workloads but also on your ability to respond to security incidents and investigate security breaches. This includes things like setting up network policies, securing the etcd database, using external security tools, and more.</p> <p>Read through the Certified Kubernetes Security Specialist (CKS) page for Domain and Competency details and information on how to register for the exam.</p> <p>Some of the hands on activities that you should be comfortable with are:</p> <ul> <li>Using kernel hardening tools like AppArmor and seccomp</li> <li>Using Falco to monitor runtime security</li> <li>Using CIS benchmarks to secure a cluster</li> <li>Performing static analysis of workloads and container images</li> <li>Implementing end-to-end traffic encryption with Cilium</li> <li>Using network policies to restrict traffic to applications</li> <li>Using Pod security standards to secure workloads </li> <li>Securing the kube-apiserver</li> </ul>"},{"location":"cks/#scanning-images-for-vulnerabilities","title":"Scanning images for vulnerabilities","text":"<p>Scanning images for vulnerabilities is an important part of securing your Kubernetes cluster. There are a number of tools that can be used to scan images for vulnerabilities. One such tool is Trivy.</p>"},{"location":"cks/#using-trivy","title":"Using Trivy","text":"<p>Trivy is a simple and comprehensive vulnerability scanner. It can be used to scan container images for vulnerabilities and generate reports. It is important to ensure that images with vulnerabilities are resolved as soon as possible to prevent security breaches.</p> <p>Trivy isn't typically installed on Kubernetes nodes. It is more commonly used in CI/CD pipelines to scan images before they are deployed to a cluster. But we installed it on the control node for the purpose of this workshop.</p> <p>Let's use Trivy on the control node and scan the <code>nginx:latest</code> image for vulnerabilities.</p> <p>SSH into the control node then run the following command to scan the <code>nginx:latest</code> image for vulnerabilities with a severity of HIGH or CRITICAL.</p> <pre><code>trivy image --severity HIGH,CRITICAL nginx:latest\n</code></pre> <p>Warning</p> <p>This can take a few minutes to complete as trivy needs to download the vulnerability database.</p> <p>After the scan has completed, you'll see that the image has a few vulnerabilities and some of which are marked <code>will_not_fix</code>. You can filter out the <code>will_not_fix</code> vulnerabilities by running the following command.</p> <pre><code>trivy image --severity HIGH,CRITICAL --ignore-status will_not_fix nginx:latest\n</code></pre> <p>There are no vulnerabilities in the <code>nginx:latest</code> image that are marked as <code>will_not_fix</code>. But in the event you do see vulnerabilities that are fixable, you can refer to the Fixed Version column to see what version of the package the vulnerability was fixed in. Typically, remediations can be applied by using newer versions of images if you are using third party images or by updating the base images in your Dockerfiles if you are building your own images.</p> <p>Trivy has the ability to scan more than just Docker images. It can also scan configuration files and other artifacts like file systems, infrastructure as code files like Terraform, sbom files, and more.</p> <p>Reference: https://trivy.dev/v0.57/tutorials/overview/</p>"},{"location":"cks/#network-policies","title":"Network Policies","text":"<p>I can't stress this enough, being proficient with network policies is critical to passing all the Kubernetes exams. Network policies are used to control the flow of traffic to and from pods. They are a powerful tool to secure your workloads and are a key component of securing a Kubernetes cluster. We did a bit of network policy exercises in the previous sections, but it is worth noting that in addition to standard Kubernetes network policies, Cilium also provides network policy that can be used to secure network traffic in a Kubernetes cluster. </p> <p>Cilium is a powerful networking and security tool that can be used to secure your Kubernetes cluster. It provides a number of features including network policies, encryption, and more.</p>"},{"location":"cks/#using-ciliumnetworkpolicy","title":"Using CiliumNetworkPolicy","text":"<p>Create the nginx1 and nginx2 pods we'll use for testing.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx1\n  labels:\n    app: nginx1\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx1\nspec:\n  selector:\n    app: nginx1\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx2\n  labels:\n    app: nginx2\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx2\nspec:\n  selector:\n    app: nginx2\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\nEOF\n</code></pre> <p>Run the following commands to create Cilium network policies that:</p> <ol> <li>Block all incoming traffic to nginx2</li> <li>Allow nginx2 to send traffic to nginx1</li> </ol> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: deny-all\nspec:\n  endpointSelector:\n    matchLabels:\n      app: nginx2\n  ingress:\n  - fromEndpoints: []\n---\napiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: allow-nginx1-to-nginx2\nspec:\n  endpointSelector:\n    matchLabels:\n      app: nginx1\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        app: nginx2\nEOF\n</code></pre> <p>Test the network policy by trying to access the nginx2 pod from the nginx1 pod and confirm that the connection times out.</p> <pre><code>kubectl exec -it nginx1 -- curl nginx2 --connect-timeout 1\n</code></pre> <p>Now, try to access the nginx1 pod from the nginx2 pod and confirm that the connection is successful.</p> <pre><code>kubectl exec -it nginx2 -- curl nginx1\n</code></pre> <p>CiliumNetworkPolicy may not appear on the CKS exam, but the concepts in using it is similar to standard Kubernetes network policies. CiliumNetworkPolicy is a powerful tool that gives you additional control over network traffic in your cluster like applying DNS-based or layer 7 policies.</p> <p>Reference: https://docs.cilium.io/en/stable/security/policy/</p>"},{"location":"cks/#encrypting-pod-traffic-with-cilium","title":"Encrypting Pod Traffic with Cilium","text":"<p>Cilium can be used to encrypt pod traffic in a Kubernetes cluster. This is done by enabling encryption in the Cilium configuration.</p> <p>There are two types of encryption that can be used:</p> <ul> <li>WireGuard - a fast and secure VPN protocol</li> <li>IPsec - a secure network protocol suite that authenticates and encrypts the packets of data sent over a network</li> </ul> <p>By default encryption is disabled in Cilium. You can confirm using this command.</p> <pre><code>kubectl -n kube-system exec -it ds/cilium -- cilium encrypt status\n</code></pre> <p>Since we initially installed Cilium without encryption, we can enable it by running the following command.</p> <pre><code>cilium config set enable-wireguard true\n</code></pre> <p>Info</p> <p>To enable encryption in Cilium with Wireguard on install, you can run the following command.</p> <pre><code>cilium install --version 1.17.1 \\\n--set encryption.enabled=true \\\n--set encryption.type=wireguard\n</code></pre> <p>Wait a few minutes for the Pods to restart then run the following command to confirm that encryption is enabled.</p> <pre><code>kubectl -n kube-system exec -it ds/cilium -- cilium encrypt status\n</code></pre> <p>You should now see that encryption is enabled using the Wireguard protocol and the interface is <code>cilium_wg0</code>.</p> <pre><code>Encryption: Wireguard                 \nInterface: cilium_wg0\n        Public key: 5qmou/aktRjAAxocap8M+NNa5guOPRCubqqMxIa5v1A=\n        Number of peers: 2\n</code></pre> <p>To verify traffic is being sent through the encrypted interface, switch to run as the root user then run the following commands to install and run tcpdump on the cilium_wg0 interface.</p> <pre><code>apt-get update\napt-get -y install tcpdump\ntcpdump -n -i cilium_wg0\n</code></pre> <p>Warning</p> <p>Make sure you switched to the root user using the <code>sudo -i</code> command before running the commands above.</p> <p>You should see encrypted traffic being sent through the cilium_wg0 interface.</p> <pre><code>istening on cilium_wg0, link-type RAW (Raw IP), snapshot length 262144 bytes\n01:34:45.400904 IP 192.168.120.136.50597 &gt; 192.168.120.135.8472: OTV, flags [I] (0x08), overlay 0, instance 4\nIP 10.0.1.1.4240 &gt; 10.0.0.105.43484: Flags [.], ack 1345254179, win 504, options [nop,nop,TS val 3516968635 ecr 3092550065], length 0\n01:34:45.401149 IP 192.168.120.135.36724 &gt; 192.168.120.136.8472: OTV, flags [I] (0x08), overlay 0, instance 6\nIP 10.0.0.105.43484 &gt; 10.0.1.1.4240: Flags [.], ack 1, win 510, options [nop,nop,TS val 3092565428 ecr 3516952919], length 0\n01:34:49.335788 IP 192.168.120.135.37207 &gt; 192.168.120.136.8472: OTV, flags [I] (0x08), overlay 0, instance 6\nIP 10.0.0.105 &gt; 10.0.1.1: ICMP echo request, id 52988, seq 0, length 32\n01:34:49.336123 IP 192.168.120.135.36724 &gt; 192.168.120.136.8472: OTV, flags [I] (0x08), overlay 0, instance 6\nIP 10.0.0.105.43484 &gt; 10.0.1.1.4240: Flags [P.], seq 1:100, ack 1, win 510, options [nop,nop,TS val 3092569363 ecr 3516952919], length 99\n01:34:49.337391 IP 192.168.120.136.38801 &gt; 192.168.120.135.8472: OTV, flags [I] (0x08), overlay 0, instance 4\nIP 10.0.1.1 &gt; 10.0.0.105: ICMP echo reply, id 52988, seq 0, length 32\n01:34:49.337494 IP 192.168.120.136.50597 &gt; 192.168.120.135.8472: OTV, flags [I] (0x08), overlay 0, instance 4\nIP 10.0.1.1.4240 &gt; 10.0.0.105.43484: Flags [P.], seq 1:76, ack 100, win 504, options [nop,nop,TS val 3516972573 ecr 3092569363], length 75\n01:34:49.337547 IP 192.168.120.135.36724 &gt; 192.168.120.136.8472: OTV, flags [I] (0x08), overlay 0, instance 6\nIP 10.0.0.105.43484 &gt; 10.0.1.1.4240: Flags [.], ack 76, win 510, options [nop,nop,TS val 3092569365 ecr 3516972573], length 0\n01:34:50.284495 IP 192.168.120.137.41721 &gt; 192.168.120.135.8472: OTV, flags [I] (0x08), overlay 0, instance 6\nIP 10.0.2.230.41078 &gt; 10.0.0.29.4240: Flags [.], ack 3241835173, win 510, options [nop,nop,TS val 2574638965 ecr 3776558958], length 0\n01:34:50.284658 IP 192.168.120.135.56975 &gt; 192.168.120.137.8472: OTV, flags [I] (0x08), overlay 0, instance 4\nIP 10.0.0.29.4240 &gt; 10.0.2.230.41078: Flags [.], ack 1, win 504, options [nop,nop,TS val 3776588986 ecr 2574623994], length 0\n01:34:50.673277 IP 192.168.120.135.56975 &gt; 192.168.120.137.8472: OTV, flags [I] (0x08), overlay 0, instance 4\nIP 10.0.0.29.4240 &gt; 10.0.2.230.41078: Flags [.], ack 1, win 504, options [nop,nop,TS val 3776589375 ecr 2574623994], length 0\n01:34:50.674070 IP 192.168.120.137.41721 &gt; 192.168.120.135.8472: OTV, flags [I] (0x08), overlay 0, instance 6\nIP 10.0.2.230.41078 &gt; 10.0.0.29.4240: Flags [.], ack 1, win 510, options [nop,nop,TS val 2574639355 ecr 3776588986], length 0\n^C\n11 packets captured\n11 packets received by filter\n0 packets dropped by kernel\n</code></pre> <p>When done, press <code>Ctrl+C</code> to stop tcpdump then exit the root user by running the <code>exit</code> command.</p> <p>Reference: https://docs.cilium.io/en/stable/security/network/encryption/</p>"},{"location":"cks/#pod-security-standards","title":"Pod Security Standards","text":"<p>Pod security standards are a set of best practices that you can use to secure your workloads. These standards are enforced by the kube-apiserver and can be used to ensure that your workloads are secure.</p> <p>Enabling Pod Security Standards is done by labeling a namespace with a label. This label can be set to various values to enforce different levels of security. The values are:</p> <ul> <li>Privileged - No restrictions, provides the most flexibility but is the least secure</li> <li>Baseline - Prevents known privilege escalations with minimal restrictions</li> <li>Restricted - Heavily restricted Pod configuration following hardening best practices</li> </ul> <p>Reference: https://kubernetes.io/docs/concepts/security/pod-security-standards/ </p>"},{"location":"cks/#enabling-pod-security-standards","title":"Enabling Pod Security Standards","text":"<p>Run the following command to label the default namespace with the <code>pod-security.kubernetes.io/enforce</code> label set to <code>restricted</code>.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: n297\n  labels:\n    pod-security.kubernetes.io/enforce: restricted # add\nEOF\n</code></pre> <p>Create a pod that violates the Pod Security Standards.</p> <pre><code>kubectl apply -n n297 -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n    - image: nginx\n      name: nginx\n      ports:\n        - containerPort: 80\nEOF\n</code></pre> <p>You will see the pod wasn't created because the pod can allow privilege escalation.</p> <pre><code>Error from server (Forbidden): error when creating \"STDIN\": pods \"nginx\" is forbidden: violates PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"nginx\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"nginx\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"nginx\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"nginx\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n</code></pre> <p>To fix the issue, update the pod spec to meet the Pod Security Standards.</p> <pre><code>kubectl apply -n n297 -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n    - image: nginx\n      name: nginx\n      ports:\n        - containerPort: 80\n      securityContext:                      \n        allowPrivilegeEscalation: false   # disallow privilege escalation\n        capabilities:                     # drop all capabilities\n          drop:\n            - ALL\n        runAsNonRoot: true                # disallow running as root\n        seccompProfile:\n          type: RuntimeDefault            # use the default seccomp profile\nEOF\n</code></pre> <p>Reference: https://kubernetes.io/docs/tutorials/security/cluster-level-pss/</p>"},{"location":"cks/#pod-admission-policies","title":"Pod Admission Policies","text":"<p>In the CKA section of this workshop, you enabled the ValidatingAdmissionPolicy plugin on the kube-apiserver. This plugin allows you to enforce policies on pods before they are created. The policies are defined using CEL expressions and be a powerful tool to enforce your organization's security policies.</p>"},{"location":"cks/#using-validating-admission-policies","title":"Using Validating Admission Policies","text":"<p>Run the following command to ValidatingAdmissionPolicy that ensures deployment creation and updates have less than or equal to 5 replicas.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingAdmissionPolicy\nmetadata:\n  name: \"demo-policy.example.com\"\nspec:\n  failurePolicy: Fail\n  matchConstraints:\n    resourceRules:\n    - apiGroups:   [\"apps\"]\n      apiVersions: [\"v1\"]\n      operations:  [\"CREATE\", \"UPDATE\"]\n      resources:   [\"deployments\"]\n  validations:\n    - expression: \"object.spec.replicas &lt;= 5\"\nEOF\n</code></pre> <p>Next, bind the policy to a namespace and set the validation action to \"Warn\".</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingAdmissionPolicyBinding\nmetadata:\n  name: \"demo-policy-binding.example.com\"\nspec:\n  policyName: \"demo-policy.example.com\"\n  validationActions: [\"Warn\"]\n  matchResources:\n    namespaceSelector:\n      matchLabels:\n        \"kubernetes.io/metadata.name\": \"default\"\nEOF\n</code></pre> <p>Test the policy by creating a deployment with more than 5 replicas.</p> <pre><code>kubectl create deploy mynginx --image=nginx --replicas=10\n</code></pre> <p>You will see the deployment was created but since the validation action was set to \"Warn\", the deployment will be created with a warning.</p> <pre><code>Warning: Validation failed for ValidatingAdmissionPolicy 'demo-policy.example.com' with binding 'demo-policy-binding.example.com': failed expression: object.spec.replicas &lt;= 5\ndeployment.apps/mynginx created\n</code></pre> <p>Update the policy binding to \"Deny\" the deployment creation.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingAdmissionPolicyBinding\nmetadata:\n  name: \"demo-policy-binding.example.com\"\nspec:\n  policyName: \"demo-policy.example.com\"\n  validationActions: [\"Deny\"]\n  matchResources:\n    namespaceSelector:\n      matchLabels:\n        \"kubernetes.io/metadata.name\": \"default\"\nEOF\n</code></pre> <p>Delete the deployment.</p> <pre><code>kubectl delete deploy mynginx\n</code></pre> <p>Try creating the deployment again.</p> <pre><code>kubectl create deploy mynginx --image=nginx --replicas=10\n</code></pre> <p>Now you'll see the deployment was denied with a failure message.</p> <pre><code>error: failed to create deployment: deployments.apps \"mynginx\" is forbidden: ValidatingAdmissionPolicy 'demo-policy.example.com' with binding 'demo-policy-binding.example.com' denied request: failed expression: object.spec.replicas &lt;= 5\n</code></pre> <p>This was a fairly simple example of ValidatingAdmissionPolicy. To see more complex examples, checkout the link below.</p> <p>Reference: https://kubernetes.io/blog/2024/04/24/validating-admission-policy-ga/</p>"},{"location":"cks/#runtime-security","title":"Runtime security","text":"<p>Runtime security focuses on protecting your applications while they're running. This includes isolating workloads from each other and from the host system, detecting anomalous behavior, and responding to security events in real-time. Below we explore gVisor as one runtime security approach.</p>"},{"location":"cks/#sandboxing-with-gvisor","title":"Sandboxing with gVisor","text":"<p>By using different runtimes, you can isolate workloads and improve security. gVisor is a user-space kernel that provides an additional layer of security between the container and the host kernel. It is designed to be lightweight and efficient, and it provides a secure environment for running untrusted workloads.</p> <p>Note</p> <p>In the Kubernetes setup section, gVisor was installed as a runtime class and configured containerd to use it. </p> <p>SSH into a worker node, switch to the root user, then run the following command on any of the worker nodes to verify that gVisor is installed as a runtime class.</p> <pre><code>runsc --version\n</code></pre> <p>Run the following command to confirm containerd is configured to use the runsc runtime.</p> <pre><code>cat /etc/containerd/config.toml | grep runsc\n</code></pre> <p>SSH into the control node and run the following command to create a runtime class for gVisor.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: gvisor\nhandler: runsc\nEOF\n</code></pre> <p>Use the runtime class in a pod spec.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-gvisor\nspec:\n  nodeName: worker-1\n  runtimeClassName: gvisor   # Use the runtime class here\n  containers:\n  - name: nginx\n    image: nginx\nEOF\n</code></pre> <p>If you run the following command in the container, you should see that gVisor is started as the runtime.</p> <pre><code>kubectl exec -it nginx-gvisor -- dmesg\n</code></pre> <p>Reference: https://kubernetes.io/docs/concepts/containers/runtime-class/</p>"},{"location":"cks/#control-plane-component-security","title":"Control plane component security","text":""},{"location":"cks/#secrets-in-etcd","title":"Secrets in etcd","text":"<p>etcd is a distributed key-value store that is used by Kubernetes to store configuration data. It is used to store cluster state and configuration data, and it is a critical component of a Kubernetes cluster.</p> <p>Secrets are stored in etcd and are encrypted at rest. However, it is important to ensure that etcd is properly secured to prevent unauthorized access to secrets.</p> <p>Run the following command to create a secret in the default namespace.</p> <pre><code>kubectl create secret generic mydatabasecreds --from-literal=username=admin --from-literal=password=mysupersecretpassword\n</code></pre> <p>Run the following command to get the secret data out of etcd.</p> <pre><code>sudo etcdctl --cert /etc/kubernetes/pki/apiserver-etcd-client.crt --key /etc/kubernetes/pki/apiserver-etcd-client.key --cacert /etc/kubernetes/pki/etcd/ca.crt get /registry/secrets/default/mydatabasecreds \n</code></pre> <p>You will see that the secret data is there in plain text. So even if a user doesn't have access to the Kubernetes API, or has restricted permissions to view secrets, if an adversary has access to a control plane node, they can still access the secrets directly from etcd.</p>"},{"location":"cks/#encrypting-secrets-in-etcd","title":"Encrypting secrets in etcd","text":"<p>Encrypting data at rest in etcd is a good practice to prevent unauthorized access.</p> <p>To begin, SSH into the control node and switch to the root user.</p> <p>Run the following command to create a new encryption key.</p> <pre><code>BASE64_ENCODED_SECRET=$(head -c 32 /dev/urandom | base64)\n</code></pre> <p>Create a new EncryptionConfiguration file with the encryption key.</p> <pre><code>mkdir /etc/kubernetes/enc\ncat &lt;&lt;EOF &gt; /etc/kubernetes/enc/enc.yaml\napiVersion: apiserver.config.k8s.io/v1\nkind: EncryptionConfiguration\nresources:\n  - resources:\n      - secrets\n    providers:\n      - aescbc:\n          keys:\n            - name: key1\n              secret: ${BASE64_ENCODED_SECRET}\n      - identity: {} # remove this line to prevent plain text retrieval\nEOF\n</code></pre> <p>Make a copy of the existing kube-apiserver manifest.</p> <pre><code>cp /etc/kubernetes/manifests/kube-apiserver.yaml /etc/kubernetes/manifests/kube-apiserver.yaml.bak\n</code></pre> <p>Using Vim, edit the kube-apiserver manifest to include <code>- --encryption-provider-config=/etc/kubernetes/enc/enc.yaml</code> to the list of kube-apiserver command flags.</p> <pre><code>vim /etc/kubernetes/manifests/kube-apiserver.yaml\n</code></pre> <p>Next scroll down towards the bottom of the manifest and add the appropriate volume and volume mount so the encryption configuration file can be mounted in the kube-apiserver pod.</p> <pre><code>    volumeMounts:                         # this is in the container spec\n    ...\n    - mountPath: /etc/kubernetes/enc      # add these 3 lines to the bottom of the list\n      name: enc                           \n      readOnly: true                      \n    ...\n  volumes:                                # this is in the pod spec\n  ...\n  - hostPath:                             # add these 4 lines to the bottom of the list\n      path: /etc/kubernetes/enc\n      type: DirectoryOrCreate\n    name: enc\n</code></pre> <p>Save and exit the kube-apiserver manifest then wait for the kube-apiserver pod to restart.</p> <pre><code>watch crictl ps\n</code></pre> <p>After you see the kube-apiserver pod has been restarted, press <code>Ctrl+C</code> to exit the watch then run the following command to update the secret which will now be encrypted.</p> <pre><code>export KUBECONFIG=/etc/kubernetes/admin.conf\nkubectl get secrets --all-namespaces -o json | kubectl replace -f -\n</code></pre> <p>Now if you try to get the secret data out of etcd, you'll see that it is encrypted.</p> <pre><code>etcdctl --cert /etc/kubernetes/pki/apiserver-etcd-client.crt --key /etc/kubernetes/pki/apiserver-etcd-client.key --cacert /etc/kubernetes/pki/etcd/ca.crt get /registry/secrets/default/mydatabasecreds | hexdump -C\n</code></pre> <p>Reference: https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/</p>"},{"location":"cks/#additional-resources","title":"Additional Resources","text":"<ul> <li>CKS Exam Curriculum</li> <li>CKS Certification Learning Path</li> <li>Kubernetes Security Best Practices</li> <li>Community curated CKS resources</li> </ul>"},{"location":"exam/","title":"Exam tips","text":""},{"location":"exam/#overview","title":"Overview","text":"<p>Studying the fundamental concepts is important especially for the multiple-choice exams (KCNA and KCSA). The remaining three exams (CKAD, CKA, and CKS) are hands-on and require you to apply the concepts in a real-world scenario. It is important to practice often and be comfortable with navigating the Kubernetes environment using kubectl and being able to quickly reference the Kubernetes documentation as the exam is \"open book\".</p>"},{"location":"exam/#navigating-the-kubernetes-documentation","title":"Navigating the Kubernetes Documentation","text":"<p>Understanding how the Kubernetes documentation site is structured and how to use its search functionality is important. </p> <p>Note</p> <p>During the exam, you can only access kubernetes.io documentation, GitHub documentation, and your exam portal. Stack Overflow, personal blogs, and other community forums are off-limits.</p> <p>The Kubernetes documentation is located at https://kubernetes.io/docs/. The search functionality is located at the top of the page. You can search for specific topics or use the search to find specific commands or resources. The site taxonomy is located on the left side of the page and is broken down into the following sections:</p> <ul> <li>Concepts: Covers fundamental concepts; ideal for beginners.</li> <li>Tasks: Provides instructions on specific tasks; useful for practical guidance.</li> <li>Tutorials: Offers step-by-step guides for tasks.</li> <li>Reference: Contains the Kubernetes API reference for direct interaction with the API.</li> </ul> <p>Often when you need to lean on a document to help with a task, you'll find yourself in the \"Tasks\" section. The site taxonomy will also be reflected in the page URL within the search results. So be mindful of the link you click on when you are searching for help.</p> <p>The \"Concepts\" sections will provide thorough explanations of the fundamental concepts of Kubernetes. This is good content to lean on if you need examples with explanation of how things work. Just be mindful of the time you spend reading through the documentation. You want to be able to quickly find the information you need and move on.</p>"},{"location":"exam/#practice-practice-practice","title":"Practice, Practice, Practice","text":"<p>With the hands-on exams, practice is key. You will spend all your time in a terminal. Therefore, being proficient with the bash shell is crucial for success. Make sure you are comfortable with the following tools:</p> <ul> <li>vim</li> <li>sed</li> <li>systemctl</li> <li>journalctl</li> <li>ps</li> </ul> <p>Use resources like this vim cheat sheet to build muscle memory for essential commands.</p> <p>These are tools that will help you navigate the environment and troubleshoot issues. You should also be comfortable with:</p> <ul> <li>redirecting output to files</li> <li>using pipes to grep output and search for specific information especially when troubleshooting</li> <li>using <code>kubectl explain</code> to understand resources</li> </ul> <p>With your local Kubernetes cluster, you should try to go through all the \"Tutorials\" found in the Kubernetes documentation. </p> <p>Also, when you sign-up for the exam, you'll gain access to killer.sh which is a platform that will allow you to take practice exams in a timed environment. You will get two opportunities to take the practice exam and you should take advantage of this. But be strategic about it. Don't take the practice exam until you've studied the material. Practice in a local kubernetes environment as much as possible, then take a practice exam when you feel like you are ready. The practice exam will give you step-by-step solutions so you can use that as additional study material. My advice is to save the last practice exam for a few days before the actual exam. This will give you good practice and you can use the solutions as a refresher.</p>"},{"location":"exam/#exam-day","title":"Exam Day","text":"<p>The tests are proctored remotely so it is important to log into the exam platform 15-20 minutes early to make sure everything is up to snuff. You will need to have a webcam and microphone so the proctor can see you and hear you. The proctors will ask you to show them your entire room to verify you're alone and have no unauthorized materials. Ensure you have a clean workspace and a camera that can be easily moved around for this purpose.</p> <p>During the exam, time will be a major factor. These exams are designed in a way that it is nearly impossible to complete all the questions in the time allotted. So you'll need to be able to quickly identify the questions you can answer and move on from the ones you can't. This is where practice comes in. The more you practice, the more you'll be able to quickly identify the questions you can answer and move on from the ones you can't. Don't be discouraged if you don't get to answer every single question. I've cleared exams where I didn't answer every question so it is possible to pass without answering every question. Lastly, every keystroke matters so be familiar with short resource names and be able to quickly type them out. For example, <code>kubectl get pods -n kube-system</code> can be shortened to <code>k get po -n kube-system</code> (after setting up the kubectl alias).</p> <p>If you have the luxury of using a large monitor, I would recommend it. This will allow you to have multiple browser windows and multiple terminal windows open at the same time. This will allow you to quickly reference the Kubernetes documentation and the exam questions at the same time.</p> <p>By following these guidelines and dedicating ample practice time, you'll be well-prepared for the Kubernetes certification exams.</p>"},{"location":"kcna/","title":"Kubernetes and Cloud Native Associate (KCNA)","text":""},{"location":"kcna/#overview","title":"Overview","text":"<p>The KCNA exam is considered to be the entry level exam. It is a multiple choice exam that tests your overall understanding of fundamental Kubernetes and Cloud Native architecture concepts.</p> <p>Read through the Kubernetes and Cloud Native Associate (KCNA) page for Domain and Competency details and information on how to register for the exam.</p> <p>Some of the topics that you should be familiar with include:</p> <ul> <li>Container fundamentals</li> <li>Kubernetes fundamentals</li> <li>Cloud Native architecture</li> <li>Cloud Native observability tools</li> <li>Cloud Native application delivery tools</li> </ul> <p>Let's practice with some exercises to help familiarize yourself with the concepts.</p>"},{"location":"kcna/#container-fundamentals","title":"Container fundamentals","text":"<p>A container is an application packaging method that allows you to run an application and its dependencies in an isolated environment. Containers are lightweight and portable, making them ideal for deploying applications in a cloud native environment.</p> <p>A container image is defined within a Dockerfile. The Dockerfile contains instructions on how to build the image. The image is then used to create a container.</p> <p>Warning</p> <p>This section will require you to work in your local machine (not in the virtual machine) and have Docker and Node.js installed on your local machine.</p> <p>If you don't have Docker installed, follow the instructions on the Docker installation page to install Docker.</p> <p>If you don't have Node.js installed, follow the instructions on the Node.js installation page to install Node.js.</p>"},{"location":"kcna/#create-a-simple-nodejs-application","title":"Create a simple Node.js application","text":"<p>Let's create a simple Node.js application and package it in a container. Open a terminal and follow the steps below:</p> <p>On your local machine, create a new directory for the application:</p> <pre><code>mkdir myapp\ncd myapp\n</code></pre> <p>Initialize a new Node.js application.</p> <pre><code>npm init -y\n</code></pre> <p>Install the <code>express</code> package.</p> <pre><code>npm install express\n</code></pre> <p>Create a new directory called <code>src</code>. This is where application source code will live.</p> <pre><code>mkdir src\ncd src\n</code></pre> <p>Create a file called <code>server.js</code> and add the following code.</p> <pre><code>const express = require(\"express\");\nconst app = express();\nconst server = app.listen(3000, () =&gt; {\n  console.log(`Express running \u2192 PORT ${server.address().port}`);\n});\napp.get(\"/\", (req, res) =&gt; {\n  res.send(\"Hello World!\");\n});\n</code></pre> <p>Return to the <code>myapp</code> directory and run the application.</p> <pre><code>cd ..\nnode src/server.js\n</code></pre> <p>Open a browser and navigate to http://localhost:3000. You should see \"Hello World!\" displayed.</p> <p>Press <code>Ctrl+C</code> to stop the application.</p>"},{"location":"kcna/#package-the-application-in-a-container","title":"Package the application in a container","text":"<p>Now that we have a simple Node.js application, let's package it in a container. We will use Docker to build the container image.</p> <p>Create a new file called <code>Dockerfile</code> in the <code>myapp</code> directory and add the following code.</p> <pre><code>FROM node:22\nWORKDIR /usr/src/app\nCOPY package*.json ./\nRUN npm install\nEXPOSE 3000\nCOPY . .\nCMD [\"node\", \"src/server.js\"]\n</code></pre> <p>Build the container image.</p> <pre><code>docker build -t myapp:latest .\n</code></pre> <p>Run the application in a container.</p> <pre><code>docker run -d -p 3000:3000 myapp:latest\n</code></pre> <p>Open a browser and navigate to http://localhost:3000. You should see \"Hello World!\" displayed.</p> <p>Run the following command to list the running containers.</p> <p><pre><code>docker ps\n</code></pre> Run the following command to stop the container.</p> <pre><code>docker stop &lt;CONTAINER_ID&gt; # replace &lt;CONTAINER_ID&gt; with the actual container ID\n</code></pre> <p>Tip</p> <p>You don't need to pass in the entire container ID. You can just pass in the first few characters of the container ID.</p>"},{"location":"kcna/#multi-stage-builds","title":"Multi-stage builds","text":"<p>The Dockerfile above is fairly simple, but it can be optimized using multi-stage builds. Multi-stage builds allow you to use multiple <code>FROM</code> statements in a single Dockerfile. This can help reduce the size of the final image and is often used as a best practice in terms of security and performance.</p> <p>Reference: https://docs.docker.com/get-started/docker-concepts/building-images/multi-stage-builds/</p> <p>Run the following command to list the container images.</p> <pre><code>docker images\n</code></pre> <p>Note the size of the <code>myapp:latest</code> image is a little over 1.6GB. It is quite large because it contains all the dependencies and build artifacts.</p> <p>Open the <code>Dockerfile</code> and replace the contents with the following code.</p> <pre><code># Build stage\nFROM node:22 AS build\nWORKDIR /usr/src/app\nCOPY package*.json ./\nRUN npm install\nCOPY . .\n\n# Production stage\nFROM node:22-slim\nWORKDIR /usr/src/app\nCOPY --from=build /usr/src/app .\nEXPOSE 3000\nCMD [\"node\", \"src/server.js\"]\n</code></pre> <p>This Dockerfile has two stages: a build stage and a production stage. The build stage is used to install dependencies and build the application, while the production stage is used to run the application using a \"slim\" version of the Node.js image. This often results in a smaller final image size and reduces the attack surface.</p> <p>Build the container image again.</p> <pre><code>docker build -t myapp:latest .\n</code></pre> <p>List the container images again.</p> <pre><code>docker images\n</code></pre> <p>The size of the <code>myapp:latest</code> image is now a little over 340MB. This is a significant reduction in size. Not only that, but the final image only contains the files needed to run the application, which is a good security practice.</p> <p>If you build and run the container image using the same commands as before, you should see the same \"Hello World!\" message displayed.</p>"},{"location":"kcna/#multi-architecture-builds","title":"Multi-architecture builds","text":"<p>The promise of containers is that they can run anywhere. But in reality, this isn't always the case. Different operating system architectures require different container images. For example, a container image built for AMD64 architecture won't run on ARM64 architecture.</p> <p>Good news is that Docker gives you the ability to build container images for different architectures. The docker CLI includes a plugin feature called buildx that allows you to build multi-arch images. This allows you to run the same container image on different platforms without modification.</p> <p>You can create a new buildx builder by running the following command.</p> <pre><code>docker buildx create --name mybuilder --driver docker-container --platform linux/amd64,linux/arm64,linux/arm64/v8 --bootstrap --use --node mybuilder\n</code></pre> <p>Note</p> <p>You can also configure Builders in your Docker Desktop settings.</p> <p>Reference: https://docs.docker.com/buildx/working-with-buildx/</p> <p>Now you can use the new buildx builder to create a multi-arch image.</p> <pre><code>docker buildx build --platform linux/amd64,linux/arm64 --output type=image,push=false -t myapp:latest .\n</code></pre> <p>In the build output you should see that the image is built for both AMD64 and ARM64 architectures.</p>"},{"location":"kcna/#container-registries","title":"Container registries","text":"<p>Container images can be stored in container registries, such as Docker Hub, a private Azure Container Registry, or GitHub Container Registry. This allows you to share images with others and deploy them to different environments.</p> <p>There are many guides available on how to push and pull images from different container registries. But for this exercise, we'll use ttl.sh which is an ephemeral and anonymous container registry offered by friends at Replicated.</p> <p>Container images pushed to ttl.sh have a limited lifetime and are automatically deleted after a specified period. So, you need to to give the image a unique repository name and give it a lifetime in the form of a tag. The tag is a string that represents the lifetime of the image. For example, <code>4h</code> means the image will be deleted after 4 hours.</p> <p>Run the following commands to create a unique repository name and tag.</p> <pre><code>IMG_REPO=$(uuidgen | tr '[:upper:]' '[:lower:]')\nIMG_VERSION=4h\n</code></pre> <p>Build and push the container image to ttl.sh.</p> <pre><code>docker buildx build --platform linux/amd64,linux/arm64 --push -t ttl.sh/${IMG_REPO}:${IMG_VERSION} .\n</code></pre> <p>Now if you run the following command, you should see the image hosted ttl.sh supports both AMD64 and ARM64 architectures.</p> <pre><code>docker buildx imagetools inspect ttl.sh/${IMG_REPO}:${IMG_VERSION}\n</code></pre> <p>Run the following command to print the container image you pushed to ttl.sh and copy the output of the command to a text file for future reference.</p> <pre><code>echo ttl.sh/${IMG_REPO}:${IMG_VERSION}\n</code></pre> <p>Note</p> <p>Remember this container image will be deleted after 4 hours.</p>"},{"location":"kcna/#kubernetes-fundamentals","title":"Kubernetes fundamentals","text":"<p>Now that you have a basic understanding of containers, let's move on to Kubernetes. Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications.</p>"},{"location":"kcna/#architecture-and-components","title":"Architecture and components","text":"<p>Quickly review the Kubernetes Architecture documentation to understand the key components of a Kubernetes cluster. It is a great way to get started with Kubernetes and will help you understand the basic concepts.</p> <p>The cluster is made up of a control plane and one or more worker nodes. The control plane is responsible for managing the cluster and the worker nodes are responsible for running the applications.</p> <p>Note</p> <p>You created a control plane node and joined worker nodes to the cluster using the kubeadm tool in the previous section. </p> <p>Be sure to read through the control plane components and node components sections of the Kubernetes docs to understand the roles of each component.</p> <p>Kubernetes has a rich and extensive ecosystem that typically requires dedicated time to master fully. In this workshop, we'll focus on practical fundamentals by deploying our Node.js application to a Kubernetes cluster, giving you hands-on experience with key concepts that matter most for the KCNA exam.</p>"},{"location":"kcna/#create-a-namespace","title":"Create a Namespace","text":"<p>Namespaces are a way to logically separate resources in a cluster. They are intended for use in environments with many users spread across multiple teams, or projects. </p> <p>Create a Namespace called <code>myapp</code> to isolate the resources for our Node.js application by running the following command.</p> <pre><code>kubectl create namespace n345\n</code></pre> <p>Tip</p> <p>You will use kubectl to perform imperative commands to deploy the application. This is good to quickly perform a task, but using declarative configuration files in the form of YAML manifests is the best way to deploy applications in production. You will visit this more in the next few sections.</p>"},{"location":"kcna/#run-a-container-in-a-pod","title":"Run a container in a Pod","text":"<p>Containers don't run on their own in Kubernetes. They run inside a Pod. A Pod is the smallest deployable unit in Kubernetes and represents a single instance of a running process in your cluster. To run a Pod, the easiest way is to use kubectl, which is the Kubernetes command-line tool that allows you to interact with your cluster and use an imperative command to create a Pod.</p> <p>SSH back into the control node and run the following command to create a Pod that runs the Node.js application.</p> <pre><code>kubectl run myapp \\\n--namespace n345 \\\n--image=&lt;PASTE_IN_YOUR_CONTAINER_IMAGE_NAME&gt; \\\n--port=3000\n</code></pre> <p>Warning</p> <p>Make sure you are connected to your virtual machine using SSH.</p> <p>Run the following command to list the Pods in your cluster.</p> <pre><code>kubectl get pods --namespace n345\n</code></pre>"},{"location":"kcna/#expose-the-pod-with-a-service","title":"Expose the Pod with a Service","text":"<p>The Pod is running, but it isn't accessible from outside the cluster. To make it accessible, you need to expose the Pod as a Service. A Service in Kubernetes is an abstraction that defines a logical set of Pods and a policy by which to access them.</p> <p>Run the following command to expose the Pod as a Service.</p> <pre><code>kubectl expose pod myapp \\\n--namespace n345 \\\n--type=NodePort \\\n--port=80 \\\n--target-port=3000\n</code></pre> <p>Note</p> <p>Notice that we're using the <code>--type=NodePort</code> flag to expose the Pod as a NodePort Service. This means that the Service will be exposed on a port on each Node in the cluster. There are other types of Services, such as ClusterIP and LoadBalancer, and you should read the Kubernetes documentation to learn more about them.</p> <p>Run the following command to list the Services in your cluster.</p> <pre><code>kubectl get services --namespace n345\n</code></pre> <p>You will see output similar to the following:</p> <pre><code>NAME    TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\nmyapp   NodePort   10.107.207.155   &lt;none&gt;        80:31519/TCP   4s\n</code></pre> <p>The <code>myapp</code> Service is now exposed on port <code>31519</code> on your virtual machine. You can access the Node.js application by navigating to http://control:31519 in your browser.</p> <p>Warning</p> <p>Your Service port might be different than <code>31519</code>, so make sure to check the output of the <code>kubectl get services</code> command and adjust the URL accordingly. This also assumes you added the control node IP address to your <code>/etc/hosts</code> file in the previous section.</p> <p>You could also generate the URL by running the following command.</p> <pre><code>echo \"http://$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[0].address}'):$(kubectl get services myapp -n n345 -o jsonpath='{.spec.ports[0].nodePort}')\"\n</code></pre> <p>Click the URL in the terminal to access the Node.js application.</p>"},{"location":"kcna/#scale-the-application-with-deployments-and-replicasets","title":"Scale the application with Deployments and ReplicaSets","text":"<p>Running a single Pod is fine for testing, but in a production environment, you would want to run multiple instances of your application for redundancy and load balancing. In order to do this, you should deploy the application using a Deployment. A Deployment manages a ReplicaSet, which in turn manages the Pods.</p> <p>When working with Deployments, you can scale the application by increasing the number of replicas.</p> <p>Let's create a Deployment that manages the Pods and scale the application to three replicas. </p> <p>Run the following command to delete the Pod and Service you created earlier.</p> <pre><code>kubectl delete pod myapp --namespace n345 --wait=false\nkubectl delete service myapp --namespace n345 --wait=false\n</code></pre> <p>Run the following command to create a Deployment that manages the Pods.</p> <pre><code>kubectl create deployment myapp \\\n--namespace n345 \\\n--image=&lt;PASTE_IN_YOUR_CONTAINER_IMAGE_NAME&gt; \\\n--replicas=3 \\\n--port=3000\n</code></pre> <p>Run the following command to list the Deployments in your cluster.</p> <pre><code>kubectl get deployments --namespace n345\n</code></pre> <p>Run the following command to list the ReplicaSets in your cluster.</p> <pre><code>kubectl describe replicasets --namespace n345\n</code></pre> <p>Now if you check the Pods, you should see three Pods running.</p> <pre><code>kubectl get pods --namespace n345\n</code></pre> <p>Did you notice that the Pods have different names? Kubernetes automatically generates a unique name for each replica.</p>"},{"location":"kcna/#exposing-web-applications-with-ingress","title":"Exposing web applications with Ingress","text":"<p>While Services provide basic network access to Pods, Ingress offers a more sophisticated way to expose HTTP/HTTPS applications by providing routing based on hostnames and paths.</p> <p>Ingress works through two components:</p> <ol> <li>Ingress resource: The API object that defines routing rules</li> <li>Ingress Controller: The implementation that enforces these rules (you installed Ingress-Nginx Controller earlier)</li> </ol> <p>Let's expose our application using Ingress.</p> <pre><code># First create a Service for our deployment\nkubectl expose deployment myapp \\\n--namespace n345 \\\n--port=80 \\\n--target-port=3000\n\n# Then create an Ingress resource pointing to that Service\nkubectl create ingress myapp \\\n--namespace n345 \\\n--class=nginx \\\n--rule=\"myapp.example.com/*=myapp:80\"\n</code></pre> <p>Now you can access the Node.js application from your local machine using the following curl command.</p> <pre><code>curl http://control -H 'Host: myapp.example.com'\n</code></pre> <p>Warning</p> <p>Run this command from your local machine, not the virtual machine.</p>"},{"location":"kcna/#cloud-native-architecture","title":"Cloud Native architecture","text":"<p>Cloud Native architecture is a design philosophy that leverages cloud computing principles to build and run scalable applications in modern environments. It's a set of practices that help organizations deliver applications faster and more reliably.</p>"},{"location":"kcna/#twelve-factor-app","title":"Twelve-factor app","text":"<p>It will be good to understand some of the key concepts of the Twelve-factor app and how they apply to Cloud Native architecture.</p>"},{"location":"kcna/#microservices","title":"Microservices","text":"<p>One important element of the Twelve-factor app architecture is treating applications as a set of loosely coupled services. This is also known as the microservices architecture. Microservices are small, independent services that work together to form a larger application. They communicate with each other over a network, typically using HTTP or messaging protocols. With microservices, you can break down complex applications into smaller, more manageable components. This allows you to develop, deploy, and scale each component independently.</p> <p>This architectural approach is demonstrated in the AKS Store Demo app. </p> <p>Run the following command to deploy this sample application to your Kubernetes cluster.</p> <pre><code>kubectl create ns pets\nkubectl apply -n pets -f https://gist.githubusercontent.com/pauldotyu/bfa405733af19bc80c357d6aab5847c7/raw/3f0dd08fcc7a0fb1662369efd0ab47c8b4d0d533/aks-store-demo.yaml\n</code></pre> <p>Run the following command to list the Pods in the <code>pets</code> namespace.</p> <pre><code>kubectl get pods -n pets\n</code></pre> <p>The application is made up of multiple microservices. There is a store-front web app written using NodeJS that is the main user interface for a customer. Products are retrieved using a REST API provided by the product-service which is written in Rust. Orders are processed via the order-service, a REST API built with ExpressJS that sends order messages to RabbitMQ (a commonly used message broker). So each of these components is written in different languages and are deployed as separate microservices but connected together via APIs to form a complete application. </p> <p></p> <p>Access the application by navigating to the Ingress IP address in your browser. Run the following command to get the URL based on the IP address of Ingress in the <code>pets</code> namespace.</p> <pre><code>echo \"http://$(kubectl get ingress -n pets store-front -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\"\n</code></pre>"},{"location":"kcna/#cloud-native-observability-tools","title":"Cloud Native observability tools","text":"<p>Observability is the ability to understand the internal state of a system by examining the outputs of its components. In a Cloud Native environment, where applications are distributed and run across multiple containers, it's essential to have tools that provide visibility into the system.</p>"},{"location":"kcna/#the-three-pillars-of-observability","title":"The Three Pillars of Observability","text":"<p>Modern observability is built on three foundational pillars:</p> <ol> <li>Metrics: Numerical data measured over time (e.g., CPU usage, request count)</li> <li>Logs: Text records of discrete events that happened in the system</li> <li>Traces: Records of requests as they flow through distributed systems</li> </ol> <p>Understanding these pillars and how they complement each other is crucial for monitoring complex Cloud Native applications.</p> <p>Reference: https://www.techtarget.com/searchitoperations/tip/The-3-pillars-of-observability-Logs-metrics-and-traces</p>"},{"location":"kcna/#key-observability-tools","title":"Key Observability Tools","text":"<p>Metrics Collection and Visualization</p> <p>Prometheus: An open-source time-series database that collects metrics from monitored targets using a pull model. Prometheus features a powerful query language (PromQL) and is the de facto standard for Kubernetes monitoring.</p> <p>Grafana: A visualization platform that connects to various data sources including Prometheus. Grafana allows you to create customizable dashboards with alerts and notifications.</p> <p>Many Kubernetes environments use the kube-prometheus-stack Helm chart to deploy a pre-configured observability solution including Prometheus, Grafana, and Alertmanager.</p> <p>Distributed Tracing</p> <p>Jaeger: A distributed tracing system inspired by Google's Dapper paper. Jaeger helps track requests across microservices and visualize service dependencies.</p> <p>OpenTelemetry: A CNCF project that provides a unified set of APIs and libraries for collecting telemetry data including traces, metrics, and logs.</p> <p>Log Collection and Analysis</p> <p>Fluentd: A CNCF graduated project that collects, processes, and forwards logs to multiple destinations.</p> <p>Elasticsearch, Logstash, and Kibana (ELK Stack): A popular stack for log collection, processing, storage, and visualization.</p>"},{"location":"kcna/#practical-application","title":"Practical Application","text":"<p>For the KCNA exam, understanding the high-level concepts of these tools and how they contribute to observability in Cloud Native environments is important. By understanding the capabilities and use cases of these tools, you'll be better prepared for the observability section of the KCNA exam but you won't be expected to know the specifics of each tool.</p>"},{"location":"kcna/#cloud-native-application-delivery-tools","title":"Cloud Native application delivery tools","text":"<p>Application delivery is the process of getting software into the hands of users in a safe and reliable way. In a Cloud Native environment, this encompasses everything from building and packaging applications to deploying and operating them continuously.</p>"},{"location":"kcna/#continuous-integration-and-continuous-delivery-cicd","title":"Continuous Integration and Continuous Delivery (CI/CD)","text":"<p>CI/CD forms the foundation of modern application delivery, automating the build, test, and deployment processes. In Cloud Native environments, CI/CD pipelines typically involve:</p> <ol> <li>Continuous Integration: Automatically building and testing code changes</li> <li>Continuous Delivery: Automatically deploying application changes to staging environments</li> <li>Continuous Deployment: Automatically deploying to production environments</li> </ol> <p>Reference: https://www.geeksforgeeks.org/what-is-ci-cd/</p>"},{"location":"kcna/#key-application-delivery-tools","title":"Key Application Delivery Tools","text":"<p>Package Management and Deployment</p> <p>Helm: The de facto package manager for Kubernetes. Helm Charts package Kubernetes resources into reusable, versioned templates that can be easily shared and deployed. Helm simplifies complex deployments through:</p> <ul> <li>Template-based resource generation</li> <li>Release management with upgrade and rollback capabilities</li> <li>Chart repositories for sharing applications</li> </ul> <p>Kustomize: Native Kubernetes configuration customization. Unlike Helm, Kustomize doesn't use templates but overlays changes on existing YAML files through:</p> <ul> <li>Base and overlay configurations</li> <li>Resource patching without modifying original files</li> <li>Environment-specific customizations</li> </ul> <p>GitOps Tools</p> <p>GitOps is an operational framework that uses Git as the single source of truth for declarative infrastructure and applications:</p> <p>ArgoCD: A declarative GitOps continuous delivery tool that:</p> <ul> <li>Automatically synchronizes the desired state from Git to Kubernetes</li> <li>Provides visualization for application state and health</li> <li>Supports multi-cluster deployments and SSO integration</li> </ul> <p>Flux: A set of continuous delivery solutions for Kubernetes that:</p> <ul> <li>Ensures cluster state matches Git repository configurations</li> <li>Provides automatic image updates based on policies</li> <li>Offers multi-tenancy capabilities with Flux's Kustomize controller</li> </ul> <p>Build Tools</p> <p>Buildpacks: A CNCF project that transforms application source code into container images by:</p> <ul> <li>Detecting the language/framework of your app automatically</li> <li>Adding appropriate runtime dependencies</li> <li>Creating optimized OCI-compliant images without requiring Dockerfiles</li> </ul> <p>Kaniko: A tool for building container images inside Kubernetes or CI environments:</p> <ul> <li>Builds images without requiring privileged Docker daemon access</li> <li>Creates images layer-by-layer, caching layers when possible</li> <li>Supports cross-architecture builds</li> </ul> <p>Progressive Delivery</p> <p>Progressive delivery extends continuous delivery with controlled rollouts:</p> <p>Flagger: A progressive delivery operator for Kubernetes that:</p> <ul> <li>Automates canary releases, A/B testing, and blue/green deployments</li> <li>Integrates with service meshes like Istio, Linkerd, and AWS App Mesh</li> <li>Provides automated rollbacks based on metrics</li> </ul> <p>Argo Rollouts: A Kubernetes controller for progressive delivery with:</p> <ul> <li>Advanced deployment strategies beyond basic Kubernetes deployments</li> <li>Traffic shaping capabilities through integration with service meshes</li> <li>Analysis capabilities to automate promotion or rollback</li> </ul> <p>For the KCNA exam, understanding how these tools fit into the Cloud Native ecosystem is important. In practice, organizations typically combine multiple tools:</p> <ul> <li>A CI system like GitHub Actions or Jenkins builds container images</li> <li>Artifacts are stored in a container registry</li> <li>Helm or Kustomize packages Kubernetes manifests</li> <li>ArgoCD or Flux applies these manifests to the cluster</li> <li>Progressive delivery tools control how new versions roll out</li> </ul> <p>Many organizations follow the GitOps model where Git repositories contain:</p> <ol> <li>Application source code</li> <li>Container build definitions</li> <li>Kubernetes manifests (often managed by Helm or Kustomize)</li> <li>Environment-specific configurations</li> </ol> <p>By understanding these tools and workflows, you'll be better prepared for the application delivery section of the KCNA exam and gain insights into how modern Cloud Native applications are delivered to production environments.</p>"},{"location":"kcna/#additional-resources","title":"Additional resources","text":"<ul> <li>KCNA Exam Curriculum</li> <li>KCNA resources for more resources.</li> <li>KCNA Certification Learning Path</li> </ul>"},{"location":"kcsa/","title":"Kubernetes and Cloud Native Security Associate (KCSA)","text":""},{"location":"kcsa/#overview","title":"Overview","text":"<p>Like the KCNA exam, the KCSA exam is an entry-level certification that focuses on Kubernetes and Cloud Native security fundamentals. This multiple-choice exam tests your understanding of essential security concepts including Kubernetes security architecture, network policies, and security best practices.</p> <p>Read through the Kubernetes and Cloud Native Security Associate (KCSA) page for Domain and Competency details and information on how to register for the exam.</p> <p>I also published a KCSA study guide that you may find helpful.</p> <p>Some of the topics that you should be familiar with are:</p> <ul> <li>4Cs of Cloud Native Security</li> <li>Kubernetes cluster component security</li> <li>Kubernetes security fundamentals</li> <li>Platform security</li> <li>Compliance and security frameworks</li> </ul>"},{"location":"kcsa/#4cs-of-cloud-native-security","title":"4Cs of Cloud Native Security","text":"<p>The 4Cs of Cloud Native Security is a framework developed by the Kubernetes community to help organizations understand the multiple layers where security must be addressed in cloud native environments. Each layer builds upon the previous one, creating a defense-in-depth security posture.</p>"},{"location":"kcsa/#code-security","title":"Code Security","text":"<p>Security at the application code level, including both your custom code and dependencies.</p> <p>Key considerations:</p> <ul> <li>Software Composition Analysis: Identify vulnerabilities in dependencies using tools like Dependabot, Snyk, or OWASP Dependency-Check</li> <li>Static Application Security Testing (SAST): Analyze source code for security issues before deployment</li> <li>Dynamic Application Security Testing (DAST): Test running applications for vulnerabilities</li> <li>Secure coding practices: Follow principles like input validation, proper error handling, and least privilege</li> </ul> <p>Example tools: SonarQube, GitHub Advanced Security, Checkmarx</p>"},{"location":"kcsa/#container-security","title":"Container Security","text":"<p>Security of container images, their contents, build process, and runtime behavior.</p> <p>Key considerations:</p> <ul> <li>Minimal base images: Use distroless or minimal images to reduce attack surface</li> <li>Image scanning: Scan container images for vulnerabilities before deployment</li> <li>No root: Run containers as non-root users whenever possible</li> <li>Immutability: Treat containers as immutable and avoid runtime changes</li> <li>Signed images: Use tools like Cosign to sign and verify container images</li> </ul> <p>Example tools: Trivy, Clair, Docker Scout, Notary</p>"},{"location":"kcsa/#cluster-security","title":"Cluster Security","text":"<p>Security of the Kubernetes platform itself, including all components and configurations.</p> <p>Key considerations:</p> <ul> <li>Authentication &amp; authorization: Implement strong RBAC policies</li> <li>Network policies: Restrict pod-to-pod communication</li> <li>Secrets management: Securely store and manage secrets</li> <li>CIS benchmarks: Follow Kubernetes security benchmarks</li> <li>Pod security standards: Implement appropriate pod security standards</li> <li>Audit logging: Enable and monitor audit logs</li> </ul> <p>Example tools: Kyverno, OPA Gatekeeper, Falco, kubesec</p>"},{"location":"kcsa/#cloud-security","title":"Cloud Security","text":"<p>Security of the underlying infrastructure provided by the cloud platform.</p> <p>Key considerations:</p> <ul> <li>IAM configuration: Implement proper identity and access management</li> <li>Network security: Configure VPCs, security groups, and firewalls</li> <li>Encryption: Enable encryption at rest and in transit</li> <li>Compliance: Adhere to relevant compliance frameworks (SOC2, PCI-DSS, etc.)</li> <li>Resource policies: Implement resource policies to prevent misconfigurations</li> </ul> <p>Example tools: AWS Security Hub, Azure Security Center, Google Security Command Center</p>"},{"location":"kcsa/#applying-the-4cs-model","title":"Applying the 4Cs Model","text":"<p>To effectively use the 4Cs model:</p> <ol> <li>Assess each layer for security gaps independently</li> <li>Prioritize issues based on risk and potential impact</li> <li>Implement security controls at each layer</li> <li>Monitor continuously using appropriate tools for each layer</li> </ol> <p>Remember that security issues in lower layers can undermine security at higher layers. For example, even the most secure application code can be compromised if running in a container.</p> <p>Reference: https://kubernetes.io/docs/concepts/security/</p>"},{"location":"kcsa/#kubernetes-cluster-component-security","title":"Kubernetes cluster component security","text":"<p>Kubernetes is a complex system with many components. Each component has its own security considerations. Here are some of the key components and their security considerations:</p> <ul> <li>kube-apiserver: The API server is the front-end for the Kubernetes control plane. It is responsible for serving the Kubernetes API and handling requests from clients. The API server should be secured using TLS and authentication.</li> <li>kubelet: The kubelet is the primary \"node agent\" that runs on each node in the cluster. It is responsible for managing the state of the node and the containers running on it. It also communicates with the API server. Therefore it is important to disable anonymous authentication and use either webhook or node authorization mode to secure access to the kubelet's HTTPS endpoint.</li> <li>etcd: The etcd database is used to store the state of the Kubernetes cluster. It is important to secure etcd using TLS and authentication. It is also important to restrict access to etcd to only the components that need it. You can also implement encryption at rest for etcd data.</li> </ul> <p>Reference: https://kubernetes.io/docs/concepts/security/cluster-components/</p>"},{"location":"kcsa/#kubernetes-security-fundamentals","title":"Kubernetes security fundamentals","text":"<p>Let's practice with some exercises to help familiarize yourself with the concepts.</p>"},{"location":"kcsa/#managing-secrets","title":"Managing secrets","text":"<p>Kubernetes Secrets are a way to store sensitive information. Secrets are stored in etcd and are base64 encoded (not encrypted); which is a key consideration when using them. Anyone with access to the etcd database can read the secrets. Therefore, it is important to use encryption at rest for etcd data.</p> <p>You can use kubectl to create a secret from a file or literal value. </p> <p>Create a secret from a file.</p> <p><pre><code>echo \"mysupersecretpassword\" &gt; /tmp/mysecret\nkubectl create secret generic my-secret-from-file --from-file=/tmp/mysecret\n</code></pre> Or to create a secret from a literal value.</p> <p><pre><code>kubectl create secret generic my-secret-from-literals --from-literal=username=admin --from-literal=password=mysupersecretpassword\n</code></pre> You can also create a secret from an environment variable file.</p> <pre><code>echo \"password=mysupersecretpassword\" &gt; /tmp/myenvfile\nkubectl create secret generic my-secret-from-env --from-env-file=/tmp/myenvfile\n</code></pre> <p>To view the secret, you can use the following command.</p> <pre><code>kubectl get secret my-secret-from-env -o json\n</code></pre> <p>Remember the secret data is base64 encoded. To decode the secret, you can use the following command.</p> <pre><code>kubectl get secret my-secret-from-env -o jsonpath='{.data.password}' | base64 --decode\n</code></pre> <p>Reference: https://kubernetes.io/docs/tasks/configmap-secret/managing-secret-using-kubectl/</p>"},{"location":"kcsa/#securing-network-traffic","title":"Securing network traffic","text":"<p>In Kubernetes, all Pods can communicate with each other by default, creating potential security risks. Network Policies is essential for controlling this traffic by specifying how groups of Pods are allowed to communicate. They effectively act as a firewall for Layer 3 and Layer 4 traffic in your Kubernetes cluster.</p> <p>Let's practice implementing these controls.</p> <p>Create two nginx deployments and expose them.</p> <pre><code>kubectl create namespace n287\nkubectl create deploy nginx1 --image=nginx --namespace n287\nkubectl create deploy nginx2 --image=nginx --namespace n287\nkubectl expose deploy nginx1 --port=80 --namespace n287\nkubectl expose deploy nginx2 --port=80 --namespace n287\n</code></pre> <p>By default, network traffic is wide open within a cluster. So if you exec into nginx1 Pod and try to curl to nginx2, you'll get a 200 response.</p> <pre><code>POD_NAME=$(kubectl get pod --namespace n287 -l app=nginx1 -o jsonpath='{.items[0].metadata.name}')\nkubectl exec -it $POD_NAME --namespace n287 -- curl -IL nginx2\n</code></pre> <p>Now let's create a NetworkPolicy that denies all ingress traffic to nginx2.</p> <pre><code>kubectl apply --namespace n287 -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-all\n  namespace: n287\nspec:\n  podSelector:\n    matchLabels:\n      app: nginx2\n  policyTypes:\n  - Ingress\nEOF\n</code></pre> <p>Note</p> <p>The command above uses a combination of <code>kubectl apply</code> and <code>-f -</code> to create a resource from stdin. This is a common pattern used in Kubernetes tutorials and documentation to create declarative resources without creating a separate YAML file but rather using a heredoc.</p> <p>Now if you try to curl nginx2 from nginx1, you'll get a connection timeout.</p> <pre><code>kubectl exec -it $POD_NAME --namespace n287 -- curl -IL nginx2 --connect-timeout 1\n</code></pre> <p>Create a NetworkPolicy that allows ingress traffic to nginx2 from nginx1.</p> <pre><code>kubectl apply --namespace n287 -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-nginx1\n  namespace: n287\nspec:\n  podSelector:\n    matchLabels:\n      app: nginx2\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: nginx1\nEOF\n</code></pre> <p>Now if you try to curl nginx2 from nginx1, you'll get a 200 response.</p> <p>For more NetworkPolicy practice, check out the NetworkPolicy Editor by Isovalent for an interactive experience.</p> <p>Note</p> <p>NetworkPolicies require a compatible Container Network Interface (CNI) plugin. Not all Kubernetes distributions support NetworkPolicies by default. Common CNI plugins that support NetworkPolicies include Calico, Cilium, and Weave Net.</p>"},{"location":"kcsa/#platform-security","title":"Platform security","text":"<p>Platform security is the security of the underlying infrastructure that Kubernetes runs on. This includes the operating system, the container runtime, and the cloud provider. Here are some key considerations for platform security:</p> <ul> <li>Use a minimal base image for your container images. This reduces the attack surface and makes it easier to keep your images up to date.</li> <li>Use a container runtime that supports security features like seccomp, AppArmor, and SELinux. These features can help to restrict the capabilities of your containers and reduce the risk of a security breach.</li> <li>Use a runtime security tool like Falco to monitor your containers for suspicious activity. Falco can help to detect things like privilege escalation, file access, and network activity that isn't allowed by your security policies.</li> <li>Use a cloud provider that has strong security features and compliance certifications. This can help to ensure that your applications are running in a secure environment.</li> <li>Implement admission control policies to enforce security policies at the time of Pod creation. This can help to ensure that only compliant Pods are allowed to run in your cluster.</li> </ul> <p>Reference: https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/</p>"},{"location":"kcsa/#compliance-and-security-frameworks","title":"Compliance and security frameworks","text":"<p>Compliance and security frameworks are a set of guidelines and best practices that help organizations to secure their Kubernetes clusters. </p> <p>Some of the most popular frameworks include:</p> <ul> <li>CIS Kubernetes Benchmark: A set of best practices for securing Kubernetes clusters. The benchmark includes recommendations for securing the API server, etcd, kubelet, and other components.</li> <li>NIST Cybersecurity Framework: A set of guidelines for managing cybersecurity risk. The framework includes recommendations for identifying, protecting, detecting, responding to, and recovering from cybersecurity incidents.</li> <li>PCI DSS: A set of security standards for organizations that handle credit card information. The standards include requirements for securing the network, applications, and data.</li> <li>HIPAA: A set of regulations for protecting the privacy and security of health information. The regulations include requirements for securing the network, applications, and data.</li> </ul>"},{"location":"kcsa/#additional-resources","title":"Additional Resources","text":"<p>Be sure to read through the following for more information on security best practices:</p> <ul> <li>KCSA Exam Curriculum</li> <li>Cloud Native Security </li> <li>OWASP Top 10 for Kubernetes </li> <li>KCSA study guide</li> </ul>"},{"location":"kubernetes/","title":"Install Kubernetes","text":""},{"location":"kubernetes/#overview","title":"Overview","text":"<p>This workshop intentionally uses Kubernetes v1.31.6 to provide cluster upgrade practice opportunities. While current exams are based on Kubernetes v1.32, the core concepts remain consistent between versions.</p> <p>As part of the cluster setup, you'll install containerd for the container runtime, gVisor for container isolation, Cilium as the CNI plugin, MetalLB for mimicking cloud load balancers, and Ingress-Nginx Controller - all well-documented, popular choices that you can substitute if preferred.</p>"},{"location":"kubernetes/#prepare-the-control-and-worker-nodes","title":"Prepare the control and worker nodes","text":"<p>For each of the nodes (control and workers), you'll need to install the necessary packages and configure the system. Each node will need to have the following installed: </p> <ul> <li>containerd</li> <li>gVisor</li> <li>crictl</li> <li>kubeadm</li> <li>kubelet</li> <li>kubectl</li> </ul> <p>To begin, SSH into the control node and switch to the root user.</p> <pre><code>sudo -i\n</code></pre> <p>Note</p> <p>Most of the tasks you perform will be as the root user. This is avoid having to use <code>sudo</code> for every command.</p>"},{"location":"kubernetes/#disable-swap","title":"Disable swap","text":"<p>To avoid Kubernetes data such as contents of Secret object being written to tmpfs, \"swap to disk\" must be disabled. </p> <p>Info</p> <p>See Tim Hokin's comment and the kubeadm prerequisites to better understand why swap must be disabled.</p> <p>Reference: https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory</p> <pre><code>swapoff -a\nsed -i '/\\&lt;swap\\&gt;/s/^/#/' /etc/fstab\n</code></pre> <p>Danger</p> <p>If swap is not disabled, you may also encounter issues with the kubelet service failing to start.</p>"},{"location":"kubernetes/#system-updates","title":"System updates","text":"<p>Update the system and install necessary packages.</p> <pre><code>apt-get update &amp;&amp; apt-get upgrade -y\n</code></pre>"},{"location":"kubernetes/#install-gvisor","title":"Install gVisor","text":"<p>gVisor is is used to provide an additional layer of security for containers. It uses runsc which is a lightweight, portable, and secure container runtime that implements the gVisor runtime interface.</p> <p>Install the dependencies for runsc.</p> <pre><code>apt-get install -y \\\napt-transport-https \\\nca-certificates \\\ncurl \\\ngnupg\n</code></pre> <p>Add the gVisor repository and install runsc.</p> <pre><code>curl -fsSL https://gvisor.dev/archive.key | sudo gpg --dearmor -o /usr/share/keyrings/gvisor-archive-keyring.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/gvisor-archive-keyring.gpg] https://storage.googleapis.com/gvisor/releases release main\" | sudo tee /etc/apt/sources.list.d/gvisor.list &gt; /dev/null\nsudo apt-get update &amp;&amp; sudo apt-get install -y runsc\n</code></pre> <p>Note</p> <p>runsc will be configured as the runtime for containerd in the next steps.</p> <p>Reference: https://gvisor.dev/docs/user_guide/containerd/quick_start/</p>"},{"location":"kubernetes/#install-containerd","title":"Install containerd","text":"<p>Kubernetes uses the Container Runtime Interface (CRI) to interact with container runtimes and containerd is the container runtime that it uses (it was dockerd in the past).</p> <p>Install containerd on each node from the Docker repository and configure it to use systemd as the cgroup driver.</p> <p>Reference: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-runtime</p> <p>Add docker gpg key and repository.</p> <pre><code>install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nchmod a+r /etc/apt/keyrings/docker.asc\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\\n  tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre> <p>Update packages and install containerd.</p> <pre><code>apt-get update\napt-get install containerd.io -y\n</code></pre> <p>Configure containerd to use systemd as the cgroup driver, use systemd cgroups, and use runsc as the runtime.</p> <pre><code># generate default config\nmkdir -p /etc/containerd\ncontainerd config default | tee /etc/containerd/config.toml\n# update to use systemd cgroup driver\nsed -e 's/SystemdCgroup = false/SystemdCgroup = true/g' -i /etc/containerd/config.toml\n# configure containerd to use runsc\nsed -e 's/shim_debug = false/shim_debug = true/g' -i /etc/containerd/config.toml\nsed -e '/\\[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes\\]/a \\\\n        [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runsc]\\n          runtime_type = \"io.containerd.runsc.v1\"' /etc/containerd/config.toml\n# restart containerd\nsystemctl restart containerd\nsystemctl enable containerd\n</code></pre> <p>Reference: https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd-systemd</p> <p>Update containerd to load the overlay and br_netfilter modules. This is necessary for the overlay network to work.</p> <pre><code>cat &lt;&lt; EOF | tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n</code></pre> <p>Update kernel network settings to allow traffic to be forwarded.</p> <pre><code>cat &lt;&lt; EOF | tee /etc/sysctl.d/kubernetes.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nEOF\n</code></pre> <p>Reference: https://kubernetes.io/docs/setup/production-environment/container-runtimes/#prerequisite-ipv4-forwarding-optional</p> <p>Load the kernel modules and apply the sysctl settings to ensure the changes are used.</p> <pre><code>modprobe overlay\nmodprobe br_netfilter\nsysctl --system\n</code></pre> <p>Verify containerd is running.</p> <pre><code>systemctl status containerd\n</code></pre>"},{"location":"kubernetes/#install-kubeadm-kubelet-and-kubectl","title":"Install kubeadm, kubelet, and kubectl","text":"<p>The following tools are necessary for a successful Kubernetes installation:</p> <ul> <li>kubeadm: the command to bootstrap the cluster.</li> <li>kubelet: the component that runs on all of the machines in the cluster and does things like starting pods and containers.</li> <li>kubectl: the command line tool to interact with the cluster.</li> </ul> <p>Reference: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl</p> <p>Add the Kubernetes repository.</p> <pre><code>curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | tee /etc/apt/sources.list.d/kubernetes.list\n</code></pre> <p>Update the system, install the Kubernetes packages, and hold packages so they don't get unintentionally updated.</p> <pre><code>apt-get update\napt-get install -y kubelet=1.31.6-1.1 kubeadm=1.31.6-1.1 kubectl=1.31.6-1.1\napt-mark hold kubelet kubeadm kubectl\n</code></pre> <p>Enable kubelet.</p> <pre><code>systemctl enable --now kubelet\n</code></pre>"},{"location":"kubernetes/#install-critctl","title":"Install critctl","text":"<p>CRI-O is an implementation of the Container Runtime Interface (CRI) used by the kubelet to interact with container runtimes. Install crictl by downloading the binary to the system.</p> <pre><code>export CRICTL_VERSION=\"v1.31.1\"\nexport CRICTL_ARCH=$(dpkg --print-architecture)\nwget https://github.com/kubernetes-sigs/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-${CRICTL_ARCH}.tar.gz\ntar zxvf crictl-${CRICTL_VERSION}-linux-${CRICTL_ARCH}.tar.gz -C /usr/local/bin\nrm -f crictl-${CRICTL_VERSION}-linux-${CRICTL_ARCH}.tar.gz\n</code></pre> <p>Reference: https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o</p> <p>Verify crictl is installed.</p> <pre><code>crictl version\n</code></pre>"},{"location":"kubernetes/#configure-crictl","title":"Configure crictl","text":"<p>Configure crictl to use the containerd socket by default to avoid seeing a warning each time you run crictl commands.</p> <pre><code>crictl config --set runtime-endpoint=unix:///run/containerd/containerd.sock\n</code></pre> <p>Reference: https://github.com/kubernetes-sigs/cri-tools/issues/868#issuecomment-1926494368</p> <p>Danger</p> <p>Before you move on, jump back to the Prepare the control and worker nodes section and repeat these steps for each worker node.</p>"},{"location":"kubernetes/#kubernetes-control-plane","title":"Kubernetes control plane","text":"<p>The control node is where the Kubernetes control plane components will be installed. This includes the API server, controller manager, scheduler, and etcd. The control node will also run the CNI plugin to provide networking for the cluster.</p>"},{"location":"kubernetes/#install-with-kubeadm","title":"Install with kubeadm","text":"<p>Using kubeadm, install the Kubernetes with the <code>kubeadm init</code> command. This will install the control plane components and create the necessary configuration files.</p> <p>Make sure you are back in the control node as the root user and run the following command to initialize the cluster.</p> <pre><code>kubeadm init --kubernetes-version 1.31.6 --pod-network-cidr 10.21.0.0/16 --v=5\n</code></pre> <p>Note</p> <p>This can take a few minutes to complete. Be patient and wait for the process to finish.</p> <p>Reference: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/</p> <p>Export the kubeconfig file so the root user can access the cluster.</p> <pre><code>export KUBECONFIG=/etc/kubernetes/admin.conf\n</code></pre>"},{"location":"kubernetes/#install-cilium-cni-plugin","title":"Install Cilium CNI plugin","text":"<p>Kubernetes requires a Container Network Interface (CNI) plugin to provide networking for the cluster. Cilium will be used as the CNI plugin for the cluster. Cilium is a powerful, efficient, and secure CNI plugin that provides network security, observability, and troubleshooting capabilities.</p> <p>Reference: https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/#install-the-cilium-cli</p> <p>Install the Cilium CLI.</p> <p>Note</p> <p>The Cilium CLI version will be pinned to <code>v0.16.24</code>. To get the latest version, visit the Cilium CLI releases page and update the version number.</p> <p>Set the Cilium version and architecture.</p> <pre><code>export CILIUM_VERSION=\"v0.16.24\"\nexport CILIUM_ARCH=$(dpkg --print-architecture)\n</code></pre> <p>Download the Cilium CLI binary its sha256sum and verify sha256sum to ensure the binary isn't corrupted.</p> <pre><code>curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_VERSION}/cilium-linux-${CILIUM_ARCH}.tar.gz{,.sha256sum}\nsha256sum --check cilium-linux-${CILIUM_ARCH}.tar.gz.sha256sum\n</code></pre> <p>Move binary to proper location and remove tarball file.</p> <pre><code>tar xzvf cilium-linux-${CILIUM_ARCH}.tar.gz -C /usr/local/bin\nrm cilium-linux-${CILIUM_ARCH}.tar.gz{,.sha256sum}\n</code></pre> <p>Verify the Cilium CLI is installed.</p> <pre><code>cilium version --client\n</code></pre> <p>Install Cilium CNI plugin.</p> <pre><code>cilium install --version 1.17.1\n</code></pre> <p>Run the following command to monitor the Cilium installation progress.</p> <pre><code>cilium status --wait\n</code></pre> <p>Note</p> <p>This process can take a few minutes and you'll initially see errors and warnings in the output. They will eventually resolve so be patient.</p> <p>Within a few minutes, you should see output like this which shows the Cilium CNI plugin is installed and running.</p> <pre><code>    /\u00af\u00af\\\n /\u00af\u00af\\__/\u00af\u00af\\    Cilium:             OK\n \\__/\u00af\u00af\\__/    Operator:           OK\n /\u00af\u00af\\__/\u00af\u00af\\    Envoy DaemonSet:    OK\n \\__/\u00af\u00af\\__/    Hubble Relay:       disabled\n    \\__/       ClusterMesh:        disabled\n\nDaemonSet              cilium-envoy       Desired: 1, Ready: 1/1, Available: 1/1\nDaemonSet              cilium             Desired: 1, Ready: 1/1, Available: 1/1\nDeployment             cilium-operator    Desired: 1, Ready: 1/1, Available: 1/1\nContainers:            cilium             Running: 1\n                       cilium-envoy       Running: 1\n                       cilium-operator    Running: 1\nCluster Pods:          0/2 managed by Cilium\nHelm chart version:\nImage versions         cilium             quay.io/cilium/cilium:v1.16.0@sha256:46ffa4ef3cf6d8885dcc4af5963b0683f7d59daa90d49ed9fb68d3b1627fe058: 1\n                       cilium-envoy       quay.io/cilium/cilium-envoy:v1.29.7-39a2a56bbd5b3a591f69dbca51d3e30ef97e0e51@sha256:bd5ff8c66716080028f414ec1cb4f7dc66f40d2fb5a009fff187f4a9b90b566b: 1\n                       cilium-operator    quay.io/cilium/operator-generic:v1.16.0@sha256:d6621c11c4e4943bf2998af7febe05be5ed6fdcf812b27ad4388f47022190316: 1\n</code></pre>"},{"location":"kubernetes/#cluster-verification","title":"Cluster verification","text":"<p>Exit the root shell and run the following commands to configure kubectl for your normal user account.</p> <pre><code>exit\n</code></pre> <p>Configure kubectl for your normal user account.</p> <pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre> <p>Verify you can connect to the cluster.</p> <pre><code>kubectl get nodes\n</code></pre> <p>You should see the control node listed as Ready. This means the CNI plugin is running and the control node is ready to accept workloads.</p> <p>Print the join command and run it on the worker nodes.</p> <pre><code>kubeadm token create --print-join-command\n</code></pre> <p>Danger</p> <p>Copy the join command to your clipboard or a text file as you'll need it for the next step.</p>"},{"location":"kubernetes/#join-the-worker-nodes","title":"Join the worker nodes","text":"<p>Log into each worker node, make sure you are in the root shell, and paste and run the join command that you copied from the control node.</p> <p>Reference: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#join-nodes</p> <p>If your shell doesn't show <code>root@worker</code>, run <code>sudo -i</code> to switch to the root user.</p> <p>Run a command similar to the following on each worker node.</p> <pre><code>kubeadm join 172.16.25.132:6443 --token xxxxxxxxxxxxxxx --discovery-token-ca-cert-hash sha256:xxxxxxxxxxxxxxxxx\n</code></pre> <p>After the worker nodes have joined the cluster, log back into the control node and verify the nodes are listed as Ready.</p> <pre><code>kubectl get nodes -w\n</code></pre> <p>Wait until all the worker nodes are listed as Ready then press <code>Ctrl+C</code> to exit the watch.</p> <p>Note</p> <p>At this point you can log out of the worker nodes and probably don't need to log into them again. You can work with cluster from the control node.</p>"},{"location":"kubernetes/#install-and-configure-tools","title":"Install and configure tools","text":"<p>Log back into the control node as your normal user to install some handy tools and configure kubectl. </p>"},{"location":"kubernetes/#configure-kubectl-completion-and-alias","title":"Configure kubectl completion and alias","text":"<p>These configurations are optional but will make your life easier when working with Kubernetes.</p> <p>Set kubectl alias so you can use <code>k</code> instead of <code>kubectl</code> and add kubectl completion.</p> <pre><code>cat &lt;&lt; EOF | tee -a ~/.bashrc\nsource &lt;(kubectl completion bash)\nalias k=kubectl\ncomplete -o default -F __start_kubectl k\nEOF\n</code></pre> <p>Reference: https://kubernetes.io/docs/reference/kubectl/quick-reference/</p> <p>Reload the bash profile.</p> <pre><code>source ~/.bashrc\n</code></pre>"},{"location":"kubernetes/#configure-vim","title":"Configure vim","text":"<p>Vim will be the main editor you work with throughout the exams, it is best to configure .vimrc to help with editing YAML files.</p> <pre><code>cat &lt;&lt; EOF | tee -a ~/.vimrc\nset tabstop=2\nset expandtab\nset shiftwidth=2\nEOF\n</code></pre>"},{"location":"kubernetes/#install-helm","title":"Install Helm","text":"<p>Helm is a package manager for Kubernetes that helps you manage Kubernetes applications. Helm uses a packaging format called charts which are a collection of files that describe a related set of Kubernetes resources.</p> <p>Run the following commands to install Helm.</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\nrm ./get_helm.sh\n</code></pre> <p>Reference: https://helm.sh/docs/intro/install/</p>"},{"location":"kubernetes/#install-etcdctl","title":"Install etcdctl","text":"<p>etcd is the critical distributed key-value store that serves as Kubernetes' primary database, storing all cluster configuration, state, and metadata.</p> <p></p> <p>Run the following command to install etcdctl, the command-line client for interacting with etcd.</p> <pre><code>sudo apt update &amp;&amp; sudo apt install etcd-client -y\n</code></pre>"},{"location":"kubernetes/#install-trivy","title":"Install trivy","text":"<p>Trivy is a vulnerability scanner for containers and other artifacts. While typically used in CI/CD pipelines rather than directly on nodes, you'll install it on the control node to become familiar with the tool.</p> <p>Run the following commands to install the Trivy.</p> <pre><code>sudo apt install wget gnupg\nwget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | gpg --dearmor | sudo tee /usr/share/keyrings/trivy.gpg &gt; /dev/null\necho \"deb [signed-by=/usr/share/keyrings/trivy.gpg] https://aquasecurity.github.io/trivy-repo/deb generic main\" | sudo tee -a /etc/apt/sources.list.d/trivy.list\nsudo apt update\nsudo apt install trivy\n</code></pre> <p>Reference: https://trivy.dev/latest/getting-started/</p>"},{"location":"kubernetes/#install-ingress-controller","title":"Install Ingress Controller","text":"<p>Running an ingress controller in a local Kubernetes cluster can be challenging since it requires a Service of type LoadBalancer which isn't available in a local environment. However, you can use MetalLB which is a load balancer implementation for bare metal Kubernetes clusters.</p>"},{"location":"kubernetes/#configure-kube-proxy","title":"Configure kube-proxy","text":"<p>Run the following commands to edit the kube-proxy configuration to use IPVS mode and enable strict ARP for MetalLB.</p> <pre><code>kubectl get configmap kube-proxy -n kube-system -o yaml | \\\nsed -e 's/mode: \"\"/mode: \"ipvs\"/' -e 's/strictARP: false/strictARP: true/' | \\\nkubectl apply -f - -n kube-system\n</code></pre>"},{"location":"kubernetes/#install-metallb","title":"Install MetalLB","text":"<p>Run the following command to add the MetalLB repository and install MetalLB.</p> <pre><code>helm repo add metallb https://metallb.github.io/metallb\nhelm install metallb metallb/metallb --namespace metallb-system --create-namespace\n</code></pre> <p>Run the following command to create a MetalLB configuration.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: default\n  namespace: metallb-system\nspec:\n  addresses:\n  - 192.168.93.10/32 # this is the IP address of the control node\n  autoAssign: true\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: default\n  namespace: metallb-system\nspec:\n  ipAddressPools:\n  - default\nEOF\n</code></pre> <p>Danger</p> <p>Make sure you update the IP address in the configuration to match the IP address of the control node.</p> <p>Tip</p> <p>If you get an error message like the following:</p> <pre><code>Error from server (InternalError): error when creating \"STDIN\": Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.104.133.116:443: connect: connection refused\nError from server (InternalError): error when creating \"STDIN\": Internal error occurred: failed calling webhook \"l2advertisementvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-l2advertisement?timeout=10s\": dial tcp 10.104.133.116:443: connect: connection refused\n</code></pre> <p>Simply wait a few seconds and try again.</p> <p>This is equivalent to simply using NodePort services but will mimic the behavior of a LoadBalancer service and assign the ingress service an IP address.</p> <p>Reference: https://metallb.universe.tf/installation/</p>"},{"location":"kubernetes/#install-ingress-controller_1","title":"Install Ingress Controller","text":"<p>Ingress Controllers provides Layer 7 load balancing for HTTP, HTTPS, and TCP traffic, allowing external access to services within your Kubernetes cluster.</p> <p>Run the following command to install the Ingress-Nginx Controller.</p> <pre><code>helm upgrade --install ingress-nginx ingress-nginx \\\n  --repo https://kubernetes.github.io/ingress-nginx \\\n  --namespace ingress-nginx --create-namespace\n</code></pre> <p>Reference: https://kubernetes.github.io/ingress-nginx/deploy/#quick-start</p>"},{"location":"kubernetes/#take-a-snapshot","title":"Take a snapshot","text":"<p>At this point, it's recommended to take another snapshot of each virtual machine. Give it a descriptive name like after-k8s-install so you can easily revert back to this point if you break something in your cluster.</p>"},{"location":"next/","title":"What's next?","text":""},{"location":"next/#overview","title":"Overview","text":"<p>So you have this guide, a practice environment, and a plan on how you want to approach these exams. What's next?</p>"},{"location":"next/#official-resources","title":"Official Resources","text":"<ul> <li>Kubernetes Documentation</li> <li>CNCF Certification Information</li> <li>Linux Foundation Training Portal</li> </ul>"},{"location":"next/#additional-resources","title":"Additional Resources","text":"<p>There's no shortage of resources available to help you prepare for the Kubernetes certification exams. Here are a few that I recommend:</p> <ul> <li>Sander van Vugt's CKA Course - Comprehensive video training for the CKA exam</li> <li>Benjamin Muschko's CKS Crash Course - GitHub repository with CKS preparation materials</li> <li>Mumshad Mannambeth's Udemy Courses - Popular courses for CKAD, CKA, and CKS</li> </ul>"},{"location":"next/#join-the-community","title":"Join the community","text":"<p>Also, don't forget to join the Kubernetes and CNCF communities on Slack. Here, you can ask questions, get help, and share your experiences with others who are also preparing for the exams.</p> <p>Join the CNCF Slack and Kubernetes Slack workspaces to connect with the community.</p> <p>Some channels you might want to join on the CNCF Slack workspace are:</p> <ul> <li>certifications</li> </ul> <p>Some channels you might want to join on the Kubernetes Slack workspace are:</p> <ul> <li>kcna-exam-prep</li> <li>ckad-exam-prep</li> <li>cka-exam-prep</li> <li>cks-exam-prep</li> </ul>"},{"location":"next/#feedback-and-contributions","title":"Feedback and contributions","text":"<p>Hopefully this workshop gives you a good launching point to start your journey towards Kubernetes certification. Remember, the key to success is practice, practice, practice. The more you practice, the more comfortable you'll be with the exam environment and the more likely you'll be to pass the exams.</p> <p>Good luck on your journey and please feel free to reach out to me on the various social media platforms if you have any questions or need help. I'm always happy to help where I can.</p> <ul> <li>LinkedIn: Paul Yu</li> <li>BlueSky: @paulyu.dev</li> <li>GitHub: @pauldotyu</li> <li>X: @pauldotyu</li> </ul> <p>If this workshop helped you in any way, please consider heading over to the GitHub repository linked at the top right corner of this page and click the \u2b50\ufe0f button. It helps me know that this workshop is useful and worth maintaining.</p> <p>What's even better is if you feel like contributing to this workshop, please feel free to open an issue or pull request. I would love to hear your feedback and suggestions on how to improve this workshop. It doesn't have to be a big change, even a small typo fix is appreciated \ud83d\ude4f</p>"},{"location":"ubuntu/","title":"Install Ubuntu Server","text":""},{"location":"ubuntu/#overview","title":"Overview","text":"<p>I chose Ubuntu 24.02 as the Linux operating system for installing Kubernetes. Ubuntu Server is one of the most popular Linux distributions and is widely used. You can attempt to use other Linux distributions, but the steps may vary slightly. </p>"},{"location":"ubuntu/#download-ubuntu-server-image","title":"Download Ubuntu Server image","text":"<p>First, you need to download the Ubuntu Server 24.02 ISO image based on your local machine's OS architecture:</p> <ul> <li>If you're on an AMD64 machine, browse to the Ubuntu Releases site and download the 64-bit PC (AMD64) server ISO image</li> <li>If you're on an ARM64 machine, browse to the Ubuntu CD Image Releases page and download the 64-bit ARM (ARMv8/AArch64) server ISO image</li> </ul>"},{"location":"ubuntu/#install-ubuntu-server-on-vmware-fusion","title":"Install Ubuntu Server on VMware Fusion","text":"<p>I'm using a MacBook with Apple Silicon, so I'll be using VMware Fusion. If you are using VMware Workstation Pro, the steps will differ slightly.</p>"},{"location":"ubuntu/#create-new-virtual-machine","title":"Create new virtual machine","text":"<p>Open VMware Fusion and click File -&gt; New to create a new virtual machine. Double-click the Install from disc or image box in the center of the window.</p> <p></p> <p>In the Create a New Virtual Machine window, drag and drop the Ubuntu Server ISO image you downloaded into the Drag a disc image here section. Click Continue.</p> <p></p> <p>In the Finish window, click the Customize Settings button to further configure the virtual machine settings. You will be prompted to give the virtual machine a name. Name it <code>control</code> to distinguish it as the control plane node and click Save.</p> <p></p> <p>The settings window will open. Click on Hard Disk (NVMe).</p> <p></p> <p>Change the size to 25 GB then click Apply.</p> <p></p> <p>Start the virtual machine by clicking the Play button.</p> <p></p>"},{"location":"ubuntu/#ubuntu-server-installation","title":"Ubuntu Server installation","text":"<p>The virtual machine will start and you'll be presented with the Ubuntu Server installer. Press the enter key to Try or Install Ubuntu Server.</p> <p>Let's go through the installation process step-by-step.</p> <p></p>"},{"location":"ubuntu/#welcome","title":"Welcome!","text":"<p>In the Welcome step, select the Language you want to use during the installation process and press the enter key to proceed to the next step.</p> <p></p>"},{"location":"ubuntu/#keyboard-configuration","title":"Keyboard configuration","text":"<p>Review your keyboard Layout and Variant then press the enter key to proceed to the next step.</p> <p></p>"},{"location":"ubuntu/#choose-the-type-of-installation","title":"Choose the type of installation","text":"<p>Keep the Ubuntu Server option selected and press the enter key to proceed to the next step.</p> <p></p>"},{"location":"ubuntu/#network-configuration","title":"Network configuration","text":"<p>By default, the network will be configured to use DHCP. For the Kubernetes cluster nodes, you want to ensure each virtual machine has a static IP address. To do this, you need to manually configure the network settings.</p> <p>Wait for the network to configure itself and display an IP range for DHCPv4 then tab through to highlight the network interface and press the enter key to edit the network configuration.</p> <p></p> <p>In the edit screen, tab through to highlight Edit IPv4, press the return key to edit. Next press enter to change IPv4 Method from Automatic (DHCP) to Manual then press the enter key to proceed to the next step. </p> <p></p> <p>Tip</p> <p>The network interface name and DHCPv4 range may be different for you. Make note of the IP address and the CIDR notation as this will be useful in the next step.</p> <p>In the network configuration editor, enter your network details which are based on the DHCPv4 range you noted in the previous step. Here is an example of the network settings based on my initial DHCPv4 configuration:</p> <ul> <li>Subnet: <code>172.16.25.0/24</code> note the CIDR notation is <code>/24</code> as presented in the screen shot above</li> <li>Address: <code>172.16.25.132</code> make sure this is an available IP address within the subnet range</li> <li>Gateway: <code>172.16.25.2</code> this is the gateway IP address which is usually the third IP address within the subnet range</li> <li>Name servers: <code>8.8.8.8,8.8.4.4</code> these are Google's DNS servers</li> </ul> <p>Danger</p> <p>Make sure to replace the IP addresses with the correct values based on your network configuration.</p> <p>Tab through to Save and press the enter key to proceed to the next step.</p> <p></p> <p>You should now see the network configuration is set to static. Tab through to Done and press the enter key to proceed to the next step.</p>"},{"location":"ubuntu/#proxy-configuration","title":"Proxy configuration","text":"<p>Keep the proxy settings blank and press the enter key to proceed to the next step.</p> <p></p>"},{"location":"ubuntu/#ubuntu-archive-mirror-configuration","title":"Ubuntu archive mirror configuration","text":"<p>Wait for the installer to find the best mirror. Once you see \"This mirror location passed tests.\" press the enter key to proceed to the next step.</p> <p></p>"},{"location":"ubuntu/#guided-storage-configuration","title":"Guided storage configuration","text":"<p>Keep the default settings then tab through to Done and press the enter key to proceed to the next step.</p> <p></p>"},{"location":"ubuntu/#storage-configuration","title":"Storage configuration","text":"<p>Warning</p> <p>The storage device won't use the entire disk space by default. So you need to edit the storage configuration to use the maximum available space. Otherwise, you may run out of disk space after installing the necessary software and running the server for a while.</p> <p>In the storage configuration screen, tab through to highlight the <code>ubuntu-lv</code> under USED DEVICES and press the enter key to enable edit.</p> <p></p> <p>Tab through to Edit and press the enter key again to edit the storage configuration.</p> <p></p> <p>Change the size to the maximum available, then tab through to Save and press the enter key to proceed to the next step.</p> <p></p> <p>You should now see the <code>ubuntu-lv</code> device size has been increased to use the maximum available disk space. Tab through to Done and press the enter key to proceed to the next step.</p> <p></p> <p>Confirm the storage changes by highlighting the Continue option and press the enter key to proceed to the next step.</p> <p></p>"},{"location":"ubuntu/#profile-configuration","title":"Profile configuration","text":"<p>Enter your name, server name, username, and password then tab through to Done and press the enter key to proceed to the next step.</p> <p></p> <p>Tip</p> <p>Use a password that is easy and quick to type because you'll be typing it often.</p>"},{"location":"ubuntu/#upgrade-to-ubuntu-pro","title":"Upgrade to Ubuntu Pro","text":"<p>Keep the default settings of Skip for now and press the enter key to proceed to the next step.</p> <p></p>"},{"location":"ubuntu/#ssh-configuration","title":"SSH configuration","text":"<p>Press the space bar to select the Install OpenSSH server option then tab through to Done and press the enter key to proceed to the next step.</p> <p></p> <p>Danger</p> <p>This part is critical. We need this to be able to SSH into the virtual machine from your local machine.</p>"},{"location":"ubuntu/#featured-server-snaps","title":"Featured server snaps","text":"<p>Skip the snaps by tabbing through to Done and press the enter key to proceed to the next step.</p> <p></p>"},{"location":"ubuntu/#installing-system","title":"Installing system","text":"<p>You should see the installation logs as the system is installed. This will take several minutes to complete.</p> <p></p>"},{"location":"ubuntu/#installation-complete","title":"Installation complete!","text":"<p>Once the installation is complete, tab through to Reboot Now and press the enter key to reboot.</p> <p></p> <p>Note</p> <p>You will be prompted to press the enter key to remove the install disk and complete the reboot process.</p>"},{"location":"ubuntu/#clone-worker-nodes","title":"Clone worker node(s)","text":"<p>Now that you have a control node, you need to create worker nodes. The easiest way to do this is to create a full clone of it.</p>"},{"location":"ubuntu/#shut-down-virtual-machine","title":"Shut down virtual machine","text":"<p>Shut down the newly created virtual machine by clicking Virtual Machine &gt; Shut Down from the VMware Fusion menu.</p> <p></p>"},{"location":"ubuntu/#clone-virtual-machine","title":"Clone virtual machine","text":"<p>In the VMware Fusion's Virtual Machine Library window, right-click on the control virtual machine and click Create Full Clone.</p> <p></p> <p>Give the new machine a unique name and click Save. I used <code>worker-1</code> to designate that it is a worker node with a number to indicate it is the first worker node.</p> <p></p> <p>Press the Play button to start the cloned virtual machine and log in.</p> <p></p>"},{"location":"ubuntu/#change-hostname","title":"Change hostname","text":"<p>Once you've logged in to the cloned virtual machine, you'll notice the hostname is the same as the control node. Change this with the following command.</p> <pre><code>sudo hostnamectl hostname worker-1\n</code></pre> <p>Note</p> <p>Make sure to replace <code>worker-1</code> with the hostname you want to use for the worker node.</p>"},{"location":"ubuntu/#change-static-ip-address","title":"Change static IP address","text":"<p>The cloned virtual machine is also configured with the same IP address as the control node. It will need to be configured with a new static IP address. </p> <p>Run the following command to open the netplan configuration file.</p> <pre><code>sudo vim /etc/netplan/50-cloud-init.yaml\n</code></pre> <p>Tip</p> <p>Press <code>i</code> to enter Vim's insert mode then use your arrow keys to navigate to the IP address and change it to use next available IP address in the subnet range. </p> <p>Here is what the file should look like after you've made the changes.</p> <pre><code>network:\n  version: 2\n  ethernets:\n    ens160:\n      addresses:\n      - \"172.16.25.133/24\" # Change this to the next available IP address\n      nameservers:\n        addresses:\n        - 8.8.8.8\n        - 8.8.4.4\n        search: []\n      routes:\n      - to: \"default\"\n        via: \"172.16.25.2\"\n</code></pre> <p>Danger</p> <p>The IP address of my control virtual machine was <code>172.16.25.132/24</code> so the IP address of the worker-1 virtual machine should be <code>172.16.25.133/24</code>. Your IP address will be different based on your network configuration so make sure to use the next available IP address in your subnet range.</p> <p>After you've made the changes, save the file.</p> <p>Tip</p> <p>To write your changes and quit Vim, press <code>Esc</code> to exit insert mode, type <code>:wq</code> to save the file</p> <p>Apply the changes by running the following commands.</p> <pre><code>sudo netplan generate\nsudo netplan apply\n</code></pre>"},{"location":"ubuntu/#reboot-the-virtual-machine","title":"Reboot the virtual machine","text":"<p>Reboot the virtual machine to apply the changes.</p> <pre><code>sudo reboot\n</code></pre> <p>You should now have a new hostname and static IP address assigned to your cloned virtual machine.</p> <p>Warning</p> <p>If you wish to create more worker nodes, shut down the worker node and repeat the cloning steps. Just make sure each cloned node has a unique hostname and static IP address.</p>"},{"location":"ubuntu/#post-installation","title":"Post installation","text":"<p>Let's make sure the virtual machines are set up correctly.</p> <p>Note</p> <p>Make sure all virtual machines are powered on.</p>"},{"location":"ubuntu/#log-into-virtual-machine","title":"Log into virtual machine","text":"<p>Log into each virtual machine using the username and password you set up during the installation process.</p> <p>You will see the IPv4 address that has been assigned to your virtual machine. Make a note of this as you'll need it to SSH into the virtual machine from the host machine.</p> <p></p>"},{"location":"ubuntu/#ssh-into-virtual-machine","title":"SSH into virtual machine","text":"<p>With the virtual machine's IP being NAT'd from the host machine, it will be accessible from the host using the static IP address. Open a terminal on your host machine and SSH into the virtual machine.</p> <p></p> <p>When prompted, type <code>yes</code> to add the virtual machine to the list of known hosts, enter your password, and you should be logged into the virtual machine.</p>"},{"location":"ubuntu/#edit-hosts-file","title":"Edit hosts file","text":"<p>To make it easier to SSH into the virtual machines without having to remember the IP addresses, run the following command to edit your <code>/etc/hosts</code> file and include the new hostnames and IP addresses of each virtual machine you created.</p> <pre><code>sudo vim /etc/hosts\n</code></pre> <p>This is what my <code>/etc/hosts</code> file looks like after adding the hostnames and IP addresses of the virtual machines.</p> <p></p> <p>Note</p> <p>Your IP addresses may be different based on your network configuration. </p> <p>Now you can SSH into each virtual machine using hostnames.</p> <p></p> <p>Warning</p> <p>Test to make sure you can SSH into each virtual machine from the host machine.</p>"},{"location":"ubuntu/#take-a-snapshot","title":"Take a snapshot","text":"<p>At this point, it's recommended to take a snapshot of each virtual machine. This way, if you make a mistake or something goes wrong, you can revert back to a known good state.</p> <p>In the VMware Fusion Virtual Machine Library window, right-click each virtual machine name and click Snapshots. From there, click on the snapshot icon to take a snapshot. Give it a descriptive name like before-k8s-install so you know what state the virtual machine is in.</p> <p>Note</p> <p>Taking snapshots should take up about 4-5GB of disk space per virtual machine so make sure you have enough disk space available.</p>"},{"location":"vmware/","title":"Install VMware","text":""},{"location":"vmware/#overview","title":"Overview","text":"<p>This workshop uses VMware as a desktop hypervisor. I chose VMware for the simple fact that I've had some experience with VMware Player in the past. When I originally wrote my three-part blog series titled Kubernetes on your Laptop, I used a M1 Macbook Pro, so VMware Fusion is used here. If you are using Windows or Linux machines, you can use VMware Workstation Pro.</p> <p>With the Broadcom acquisition of VMware, the company has updated licensing for its products and both VMware Fusion and VMware Workstation Pro are now free for personal use. In this workshop, we're purely using the platform for study and learning purposes, and I am publishing this workshop as free and open-source content, so using the free version of VMware seems appropriate.</p> <p>If you're opposed to using VMware products, you can use VirtualBox, Hyper-V or any other desktop hypervisor that you're comfortable with. The VM creation process will be similar, but the installation steps may vary slightly. The rest of the workshop will remain the same regardless of the hypervisor you choose.</p>"},{"location":"vmware/#download-vmware-products","title":"Download VMware products","text":"<p>Start by signing up for a free Broadcom Support account at https://access.broadcom.com then log in and navigate to the Free Downloads page.</p> <p>If you are using a macOS machine with Apple silicon, download and install VMware Fusion 13.6.2.</p> <p>If you are using a Windows or Linux machine, download and install VMware Workstation Pro 17.6.2.</p> <p>Warning</p> <p>VMware Fusion 13.6.3 and VMware Workstation Pro 17.6.3 are the latest versions as of March 4, 2025. The instructions in this workshop are based on the versions listed above, so you should be using the same versions to avoid any discrepancies. </p>"},{"location":"vmware/#install-vmware-products","title":"Install VMware products","text":"<p>Installing VMware Fusion or VMware Workstation Pro is straightforward. Simply double-click the downloaded file and follow the on-screen instructions.</p> <p>Note</p> <p>Installing VMWare Workstation Pro 17.6.2 on Ubuntu Desktop can be a bit tricky, especially if you are using a UEFI-based system with secure boot enabled. I've ran into a few issues installing on my Ubuntu 24.10 machine and published a blog post titled, Installing VMware Workstation Pro 17.6.2 on Ubuntu Desktop that you can follow.</p>"}]}